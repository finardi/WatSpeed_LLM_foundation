{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/finardi/WatSpeed_LLM_foundation/blob/main/Module2%3A%20Lab_assignment_GPT_3_5_multi_document_question_answering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Module 2 - Lab assignment - GPT-3.5 prompt engineering\n",
        "\n",
        "This notebook presents an example of how to use the GPT-3.5-turbo model to perform multi-document question answering. The task consists of answering a question given a set of documents that may contain relevant information. In this lab assignment, we will focus on prompt engineering, which creates effective prompts to improve the model's performance.\n",
        "\n",
        "We will work with the GPT-3.5-turbo model, a large-scale language model based on the GPT-3 architecture. This model has been fine-tuned on a wide range of natural language processing tasks and is known for its high performance.\n",
        "\n",
        "In this notebook, we use the [Incomplete Information Reading Comprehension (IIRC)](https://allenai.org/data/iirc) dataset as a benchmark. The IIRC dataset contains questions that require the model to reason over multiple documents to generate a complete answer. It is a challenging dataset that requires the model to extract relevant information from multiple sources and synthesize it to produce an accurate answer.\n",
        "\n",
        "We will focus on prompt engineering using the CoT approach to improve the model's performance on this task. CoT involves generating a reasoning paragraph that guides the model to focus on the most relevant information when answering the question. By providing the model with a well-crafted prompt, we can improve its ability to extract and synthesize information from multiple sources, leading to more accurate and complete answers, as demonstrated in [Visconde](https://arxiv.org/pdf/2212.09656.pdf)."
      ],
      "metadata": {
        "id": "-cbUjtUTdH-h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Assignment:** search for `TODO:` in the cells and write your code to accomplish the task."
      ],
      "metadata": {
        "id": "DIeqKD0eVQlu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installing required packages\n",
        "\n",
        "In this example, we have to install `openai` and `tiktoken` libraries.\n",
        "\n",
        "**`openai`**:\n",
        "\n",
        "OpenAI is an artificial intelligence research laboratory consisting of the for-profit corporation OpenAI LP and its parent company, the non-profit OpenAI Inc. The OpenAI library is a powerful machine learning library that provides an easy-to-use interface to the OpenAI API. With this library, users can easily integrate OpenAI's state-of-the-art language models, including GPT-3, into their applications, and leverage the full power of these models to perform various natural language processing (NLP) tasks, such as language generation, classification, question-answering, and more.\n",
        "\n",
        "**`tiktoken`**:\n",
        "\n",
        "Tiktoken is an open-source BPE tokeniser developed by OpenAI that is used to split text strings into tokens. It is useful for models like GPT-3 that encode text into tokens. Tiktoken is designed to be highly efficient, capable of handling large amounts of text quickly and accurately."
      ],
      "metadata": {
        "id": "6qk2TxsAf0Uo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "yRw5m7KSH-WR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "781bdb31-762b-4874-a607-9d5b3f404454"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m767.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m49.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q openai tiktoken"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Downloading the data\n",
        "\n",
        "To use the Incomplete Information Reading Comprehension (IIRC) dataset as a benchmark, we need to download the data. The IIRC dataset consists of a set of documents and associated questions. We can download the dataset test set using the following code:\n",
        "\n"
      ],
      "metadata": {
        "id": "KSTZN-_nytrt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://iirc-dataset.s3.us-west-2.amazonaws.com/iirc_test.json"
      ],
      "metadata": {
        "id": "2A5abFLoyw1m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "885ec975-3376-4c5a-9507-109a2dd65bcc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-05-14 12:58:20--  https://iirc-dataset.s3.us-west-2.amazonaws.com/iirc_test.json\n",
            "Resolving iirc-dataset.s3.us-west-2.amazonaws.com (iirc-dataset.s3.us-west-2.amazonaws.com)... 52.92.136.178, 3.5.82.161, 52.92.152.74, ...\n",
            "Connecting to iirc-dataset.s3.us-west-2.amazonaws.com (iirc-dataset.s3.us-west-2.amazonaws.com)|52.92.136.178|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2874825 (2.7M) [application/json]\n",
            "Saving to: ‘iirc_test.json’\n",
            "\n",
            "iirc_test.json      100%[===================>]   2.74M  4.70MB/s    in 0.6s    \n",
            "\n",
            "2023-05-14 12:58:21 (4.70 MB/s) - ‘iirc_test.json’ saved [2874825/2874825]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's load the data and see what it looks like. \n",
        "\n",
        "We are using the IIRC dataset, and we have imported the JSON library to read the test set file. We loaded the first example from the test set, which is a dictionary with keys 'questions', 'text', 'links', and 'title'. \n",
        "\n",
        "The 'questions' key contains a list of dictionaries with keys 'question', 'context', 'answer', and 'question_links'. The 'text' key contains the text that may contain relevant information for answering the questions. The 'links' key is a list of dictionaries with keys 'target' and 'indices', indicating the hyperlink target and the position of the hyperlink in the text. The 'title' key contains the title of the document. \n",
        "\n",
        "In this particular example, we can see that the question is \"What is Zeus known for in Greek mythology?\" and the answer is \"being the sky and thunder god\". The context contains three passages containing the text that may provide additional information."
      ],
      "metadata": {
        "id": "Gm3ivL9yQfqv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "test_set = json.load(open('iirc_test.json','r'))\n",
        "\n",
        "test_set[0]"
      ],
      "metadata": {
        "id": "ayTFOsujy0jR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18783619-c3a2-423d-ef73-643bce80663a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'questions': [{'answer': {'type': 'span',\n",
              "    'answer_spans': [{'text': 'sky and thunder god',\n",
              "      'passage': 'zeus',\n",
              "      'type': 'answer',\n",
              "      'start': 83,\n",
              "      'end': 102}]},\n",
              "   'question': 'What is Zeus know for in Greek mythology?',\n",
              "   'context': [{'text': 'he Palici the sons of Zeus',\n",
              "     'passage': 'main',\n",
              "     'indices': [684, 710]},\n",
              "    {'text': 'in Greek mythology', 'passage': 'main', 'indices': [137, 155]},\n",
              "    {'text': 'Zeus (British English , North American English ; , Zeús ) is the sky and thunder god in ancient Greek religion',\n",
              "     'passage': 'Zeus',\n",
              "     'indices': [0, 110]}],\n",
              "   'question_links': ['Greek mythology', 'Zeus']}],\n",
              " 'text': \"The Palici (Παλικοί in Greek), or Palaci, were a pair of indigenous Sicilian chthonic deities in Roman mythology, and to a lesser extent in Greek mythology. They are mentioned in Ovid's Metamorphoses V, 406, and in Virgil's Aeneid IX, 585. Their cult centered on three small lakes that emitted sulphurous vapors in the Palagonia plain, and as a result these twin brothers were associated with geysers and the underworld. There was also a shrine to the Palaci in Palacia, where people could subject themselves or others to tests of reliability through divine judgement; passing meant that an oath could be trusted. The mythological lineage of the Palici is uncertain; one legend made the Palici the sons of Zeus, or possibly Hephaestus, by Aetna or Thalia, but another claimed that the Palici were the sons of the Sicilian deity Adranus.\\n\",\n",
              " 'links': [{'target': 'Sicily', 'indices': [68, 76]},\n",
              "  {'target': 'Chthonic', 'indices': [77, 85]},\n",
              "  {'target': 'Roman mythology', 'indices': [97, 112]},\n",
              "  {'target': 'Greek mythology', 'indices': [140, 155]},\n",
              "  {'target': 'Ovid', 'indices': [179, 183]},\n",
              "  {'target': 'Virgil', 'indices': [215, 221]},\n",
              "  {'target': 'Palagonia', 'indices': [319, 328]},\n",
              "  {'target': 'Geyser', 'indices': [393, 400]},\n",
              "  {'target': 'Zeus', 'indices': [706, 710]},\n",
              "  {'target': 'Hephaestus', 'indices': [724, 734]},\n",
              "  {'target': 'Aetna (nymph)', 'indices': [739, 744]},\n",
              "  {'target': 'Thalia (nymph)', 'indices': [748, 754]},\n",
              "  {'target': 'Adranus', 'indices': [828, 835]}],\n",
              " 'title': 'Palici'}"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preparation\n",
        "\n",
        "When working with the IIRC dataset, preparing the data before using it to evaluate models is necessary. Additionally, it is important to carefully choose how many questions to evaluate, as using GPT from OpenAI API can be costly. The data preparation process involves iterating through the test set,\n",
        "extracting relevant information such as the question, documents containing relevant passages, and the true answer.\n",
        "\n",
        "This is achieved by parsing the original JSON format and mapping it into a format that can be used to evaluate models. \n",
        "\n",
        "In the code below, the preparation process is limited to a maximum of 50 questions due to the cost associated with using the OpenAI API. It involves iterating through each question and its corresponding context, creating a list of documents containing relevant information, and extracting the true answer based on its type. By preparing the data in this way, it can be more easily fed into GPT models, and compare the results. \n",
        "\n",
        "We use a limited number of questions to reduce the costs of using OpenAPI API."
      ],
      "metadata": {
        "id": "Rp-0LjKV-DjJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_questions = 50 # @param\n",
        "\n",
        "test_set_questions = []\n",
        "i = 0 \n",
        "while len(test_set_questions) < max_questions:\n",
        "\n",
        "  item = test_set[i]\n",
        "  for question in item['questions']:\n",
        "    documents = []    \n",
        "    for doc in question['context']:\n",
        "      documents.append({\n",
        "          \"title\": doc['passage'] if doc['passage'] != \"main\" else item['title'],\n",
        "          \"content\": doc['text']\n",
        "      })\n",
        "    true_answer = \"\"\n",
        "    if question['answer']['type'] == \"span\":\n",
        "      true_answer = \", \".join([a['text'] for a in question['answer'][\"answer_spans\"]])\n",
        "    elif question['answer']['type'] == \"value\":\n",
        "        true_answer = \"{0} {1}\".format(question['answer']['answer_value'],question['answer']['answer_unit'])\n",
        "    elif question['answer']['type'] == \"binary\":\n",
        "        true_answer = question['answer']['answer_value']\n",
        "    elif question['answer']['type'] == \"none\":\n",
        "        true_answer = \"Not enough information.\"\n",
        "    test_set_questions.append({\n",
        "        \"question\": question['question'],\n",
        "        \"documents\": documents,\n",
        "        \"answer\": true_answer\n",
        "    })\n",
        "    i+=1\n"
      ],
      "metadata": {
        "id": "ObWhjm-B-GZY"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The resulting prepared dataset is a dictionary with four keys. The \"**`question`**\" key contains the actual question to be answered, which in this case is \"What is Zeus known for in Greek mythology?\". The \"**`documents`**\" key is a list containing information that may be relevant to answering the question. Each document is a dictionary with a \"title\" key and a \"content\" key. The \"title\" key gives the name of the source of the information, while the \"content\" key provides the actual text of the source. In this case, there are three documents, all related to the topic of Greek mythology and Zeus. The \"**`answer`**\" key contains the correct answer to the question, which is \"sky and thunder god\".\n",
        "\n"
      ],
      "metadata": {
        "id": "8umiH5WOSev1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_set_questions[0]"
      ],
      "metadata": {
        "id": "bUAD1vpb-g9F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5153e62-f1f9-43db-fd44-ca86ef3b4852"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'question': 'What is Zeus know for in Greek mythology?',\n",
              " 'documents': [{'title': 'Palici', 'content': 'he Palici the sons of Zeus'},\n",
              "  {'title': 'Palici', 'content': 'in Greek mythology'},\n",
              "  {'title': 'Zeus',\n",
              "   'content': 'Zeus (British English , North American English ; , Zeús ) is the sky and thunder god in ancient Greek religion'}],\n",
              " 'answer': 'sky and thunder god'}"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using OpenAI API\n",
        "\n",
        "To use OpenAI API, we need to set our API key and import the OpenAI module. In the given code, we have the `OPENAI_KEY` variable which we can set to our OpenAI API key. After that, we can use the `openai.api_key` method to set the API key for our session.\n",
        "\n",
        "The function `generate_chat` takes in a list of messages and generates a response using the OpenAI Chat API. The `model` parameter specifies which model to use for generating the response. In the given code, we have used the `gpt-3.5-turbo` model. However, we can also use `gpt-4`.\n",
        "\n",
        "The function `generate` takes in a prompt and generates a completion using the OpenAI Completion API. In the given code, we have used the `text-davinci-003` engine, which is a powerful language model that can generate human-like responses to text prompts. We can also set various parameters such as `max_tokens`, `temperature`, `top_p`, `frequency_penalty`, and `presence_penalty` to control the behavior of the model. \n",
        "\n",
        "**`top_p`**: This parameter controls the \"nucleus sampling\" algorithm used by the language model to generate text. It determines the probability threshold for including words in the generated text. Specifically, the algorithm starts with the most probable words and keeps adding them until their cumulative probability reaches `top_p`. This allows the model to generate more diverse and varied responses. The default value is 1, which means the algorithm always includes the most probable words.\n",
        "\n",
        "**`frequency_penalty`**: This parameter discourages the model from repeating the same words or phrases too often in the generated text. The higher the value, the more the model is penalized for repeating itself. The default value is 0, which means no penalty is applied.\n",
        "\n",
        "**`presence_penalty`**: This parameter encourages the model to include words or phrases from the prompt or context in the generated text. The higher the value, the more the model is penalized for generating words or phrases that are not present in the prompt or context. The default value is 0, which means no penalty is applied.\n",
        "\n",
        "**`temperature`**: Temperature is a hyperparameter in natural language processing (NLP) models like GPT-3 that controls the degree of randomness in the output of the model. A higher temperature value leads to more diverse and unpredictable responses, while a lower temperature value results in more conservative and predictable responses.\n",
        "\n",
        "**IMPORTANT:** It's important to note that there are costs associated with using the OpenAI API, so we need to choose the appropriate model and set the parameters carefully to avoid unnecessary expenses. The `gpt-3.5-turbo` model used in `generate_chat` is a cheaper option compared to `text-davinci-003`."
      ],
      "metadata": {
        "id": "u4AWQ6B2gdIi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "\n",
        "OPENAI_KEY = \"\" # @param set your OpenAI API key here\n",
        "\n",
        "openai.api_key = OPENAI_KEY\n",
        "\n",
        "def generate_chat(messages,model=\"gpt-3.5-turbo\"):\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=0\n",
        "    )\n",
        "    return response[\"choices\"][0]['message']['content']\n",
        "\n",
        "\n",
        "def generate(prompt,max_tokens=128, temperature=0):\n",
        "    response = openai.Completion.create(\n",
        "        engine=\"text-davinci-003\",\n",
        "        prompt=prompt,\n",
        "        temperature=temperature,\n",
        "        max_tokens=max_tokens,\n",
        "        top_p=1,\n",
        "        frequency_penalty=0,\n",
        "        presence_penalty=0\n",
        "    )\n",
        "\n",
        "    return response[\"choices\"][0]['text']"
      ],
      "metadata": {
        "id": "1fWxwY7kRo0H"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question Answering\n",
        "\n",
        "\n",
        "In this section, we perform the question-answering step. Using the prepared dataset, we apply the GPT-3.5 model to generate answers to the questions provided in the test set. The goal is to evaluate the performance of the model in providing accurate answers to natural language questions. This is a crucial step in natural language processing, as it enables machines to understand and respond to human queries. We will use the OpenAI API to generate the answers and evaluate the results against the true answers provided in the test set.\n",
        "\n",
        "In this example, we will use a 3-shot prompt with Chain-of-thought available [here](https://platform.openai.com/playground/p/bjGeM1rNaxzm6GANTAwNXz1l).\n",
        "\n",
        "In the following sub-sections, we'll review each part of this prompt. "
      ],
      "metadata": {
        "id": "PkX0A0m_zz-c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instruction\n",
        "\n",
        "To perform the question answering task, GPT-3 and GPT-3.5 need to be instructed on the task they have to perform. The instruction is defined in the code below.\n",
        "\n",
        "This instruction specifies that the model should use the provided documents to generate an answer to the question and provide an explanation for how it arrived at that answer. Additionally, the model should cite evidence from the documents to support its answer. If there is not enough information in the documents to answer the question, the model should indicate that with the phrase \"not enough information\". The expected format for the model's response is also defined in the instruction.\n",
        "\n",
        "```\n",
        "Expected Format:\n",
        "Explanation: <>\n",
        "Answer: <>\n",
        "```\n",
        "\n",
        "Defining the expected format is important because it provides a clear guideline for the model to follow when generating its response. By providing a clear template for the output, we can ensure that the model outputs information in a consistent and organized manner. This makes it easier for us to post-process the model's generation and extract the target information.\n",
        "\n",
        "[Best practices for prompt engineering with OpenAI API\n",
        "](https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering-with-openai-api)"
      ],
      "metadata": {
        "id": "MFA0TLo3iWih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "instruction = \"\"\"For each example, use the documents to provide an answer to the question and cite \n",
        "evidence from the documents to support the answer. If there is not enough information in the documents \n",
        "to answer the question, then state \"not enough information\".\n",
        "Expected Format:\n",
        "Explanation: <>\n",
        "Answer: <>\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "wjR7VKfTiWC7"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Few-shot examples\n",
        "\n",
        "The second part of the prompt consists of the few-shot examples. The few-shot examples are crucial in this approach because they enable the model to learn how to perform the task with only a small number of examples. The few-shot examples allow the model to \"understand\" how to answer questions based on a small set of examples, which makes it more robust and capable of answering a wider range of questions.\n",
        "\n",
        "In the code provided below, we define three multi-doc QA examples that we will use as few-shot examples. These examples will serve as inputs to the OpenAI API to generate responses for the given questions. We will provide the context documents and the question as the prompt to the API, and it will generate the answer for us. By using multi-doc examples, we allow the API to consider multiple sources of information to provide us with the best answer possible. These few-shot examples allow us to see how well the OpenAI API can perform on previously unseen questions that are similar to the examples we provide."
      ],
      "metadata": {
        "id": "Slk98B6mUiQ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "examples = [\n",
        "    {\n",
        "        \"documents\": [\n",
        "            {\"title\": \"Fred J. Shields\", \"content\": \"Ollie Murphy's first-half goal gave 'the Royals' a huge boost at half-time.\"},\n",
        "            {\"title\": \"Ollie Murphy\", \"content\": \"He plays club football for Carnaross\"},\n",
        "            {\"title\": \"Ollie Murphy\", \"content\": \"He came to national prominence in 1999 when he was one of Meath's best player's\"}\n",
        "        ],\n",
        "        \"question\": \"Did Ollie Murphy play for any teams other than 'the Royals'?\",\n",
        "        \"explanation\": \"Document 1 says that Ollie Murphy's first-half goal gave 'the Royals' a huge boost at half-time. However, this does not necessarily mean that Ollie only played for 'the Royals'. Document 2 states that Ollie plays club football for Carnaross. This suggests that Ollie may have played for other teams in addition to 'the Royals'. Document 3 says that Ollie came to national prominence in 1999 when he was one of Meath's best players. This also suggests that Ollie has played for other teams. Therefore, it is likely that Ollie has played for teams other than 'the Royals'.\",\n",
        "        \"answer\": \"yes\"\n",
        "    },\n",
        "    {\n",
        "        \"documents\": [\n",
        "            {\"title\": \"Don Rendell\", \"content\": \"The club played in the Brunei Premier League in the early 2000s, winning the league title in 2002 and 2004.\"}\n",
        "        ],\n",
        "        \"question\": \"What club came in second at the 2004 Brunei Premier League?\",\n",
        "        \"explanation\": \"Despite the document 1 being about the premier league and saying who won it in 2004, there is not enough information to determine which club came in second place during that year.\",\n",
        "        \"answer\": \"not enough information.\"\n",
        "    },\n",
        "    {\n",
        "        \"documents\": [\n",
        "            {\"title\": \"Stacy Compton\", \"content\": \"Despicable Me, the first film in the series, and the first film from Illumination, was released on July 9, 2010.\"},\n",
        "            {\"title\": \"Miranda Cosgrove\", \"content\": \"Cosgrove's first television appearance (aside from commercials) was in 2001 as the voice of young Lana Lang in the pilot episode of Smallville.\"},\n",
        "            {\"title\": \"Miranda Cosgrove\", \"content\": \"In 2004, Cosgrove soon landed her first major role in a television series when she was awarded a main role in the Nickelodeon series Drake & Josh\"},\n",
        "            {\"title\": \"Miranda Cosgrove\", \"content\": \"Also in 2004, Cosgrove guest-starred in a special episode of the animated series What's New, Scooby-Doo?, as well as guest-starring in a season five episode of Grounded For Life\"},\n",
        "            {\"title\": \"Miranda Cosgrove\", \"content\": \"The television series, which aired on Disney, is a spin-off of the original film, Lilo & Stitch\"},\n",
        "            {\"title\": \"Miranda Cosgrove\", \"content\": \"The first of these appearances was in Zoey 101. Cosgrove later guest starred on an episode of Unfabulous,\"},\n",
        "            {\"title\": \"Miranda Cosgrove\", \"content\": \"However, Cosgrove was already in the works of starring in her own sitcom, titled iCarly, released on September 8, 2007.\"}\n",
        "        ],\n",
        "        \"question\": \"How many TV shows had Miranda Cosgrove been featured in by the year Despicable Me was released?\",\n",
        "        \"explanation\": \"According to document 1, Despicable Me was released on July 9, 2010.Document 2 states that Cosgrove's first television appearance was in 2001 as the voice of young Lana Lang in the pilot episode of Smallville.Document 3 says that, in 2004, Cosgrove landed her first major role in a television series when she was awarded a main role in the Nickelodeon series Drake & Josh.Document 4 states that, also in 2004, Cosgrove guest-starred in a special episode of the animated series What's New, Scooby-Doo?, as well as guest-starring in a season five episode of Grounded For Life.Document 5 says that the television series, which aired on Disney, is a spin-off of the original film, Lilo & Stitch.Document 6 states that the first of these appearances was in Zoey 101. Cosgrove later guest starred on an episode of Unfabulous.Document 7 says that, however, Cosgrove was already in the works of starring in her own sitcom, titled iCarly, released on September 8, 2007.Therefore, Miranda Cosgrove had been featured in 8 TV shows by the year Despicable Me was released.\",\n",
        "        \"answer\": \"8 TV shows\"\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "o2DBARBtz1pJ"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining post-processing function\n",
        "\n",
        "The next code cells aim to answer the multi-document question using OpenAI API to consume two models: InstructGPT `text-davinci-003` and ChatGPT `gpt-3.5-turbo`.\n",
        "\n",
        "Considering our example prompt, the next steps consist of appending the target example to the end of the prompt.\n",
        "\n",
        "First, we have to define an \"**util**\" function responsible for extracting the explanation and the answer written by the model. The code below defines a function named `extract_explanation_and_answer`, which takes a string `text` as input. This function uses regular expressions to search the text for an \"Explanation\" and \"Answer\" section, which are expected to be formatted as \"Explanation: \\<explanation text\\>\\n\" and \"Answer: \\<answer text\\>\". It then extracts the text after the \"Explanation:\" label and before the newline character as the explanation, and the text after the \"Answer:\" label as the answer."
      ],
      "metadata": {
        "id": "lE5Rsi-qhx1u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = \"\"\" \n",
        "Explanation: Document 1 mentions that Calvin Mackie was featured on HBO as a commentator on Spike Lee's documentary on the Katrina disaster When The Levees Broke: A Requiem in Four Parts. Document 2 states that When the Levees Broke: A Requiem in Four Acts is a 2006 documentary film directed by Spike Lee about the devastation of New Orleans, Louisiana following the failure of the levees during Hurricane Katrina. It was filmed in late August and early September 2005, and premiered at the New Orleans Arena on August 16, 2006. Therefore, Spike Lee debuted his documentary When the Levees Broke;\\nAnswer: August and September of 2005\"\"\"\n",
        "extract_explanation_and_answer(a)    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E1EKuOS9emD_",
        "outputId": "98518f4e-ffe1-42df-aaa7-79f1cd3532c6"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(\"Document 1 mentions that Calvin Mackie was featured on HBO as a commentator on Spike Lee's documentary on the Katrina disaster When The Levees Broke: A Requiem in Four Parts. Document 2 states that When the Levees Broke: A Requiem in Four Acts is a 2006 documentary film directed by Spike Lee about the devastation of New Orleans, Louisiana following the failure of the levees during Hurricane Katrina. It was filmed in late August and early September 2005, and premiered at the New Orleans Arena on August 16, 2006. Therefore, Spike Lee debuted his documentary When the Levees Broke;\",\n",
              " 'August and September of 2005')"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chatGPT_output[-1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "2kfJ7gUEen1Y",
        "outputId": "811d56be-2520-4f03-9425-d6e35b2742ce"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nExplanation: Document 1 mentions that Calvin Mackie was featured on HBO as a commentator on Spike Lee's documentary on the Katrina disaster When The Levees Broke: A Requiem in Four Parts. Document 2 states that When the Levees Broke: A Requiem in Four Acts is a 2006 documentary film directed by Spike Lee about the devastation of New Orleans, Louisiana following the failure of the levees during Hurricane Katrina. It was filmed in late August and early September 2005, and premiered at the New Orleans Arena on August 16, 2006. Therefore, Spike Lee debuted his documentary When the Levees Broke;\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def extract_explanation_and_answer(text):\n",
        "    # Define regular expressions to match the explanation and answer\n",
        "    explanation_regex = r\"Explanation:\\s(.+?)\\n\"\n",
        "    answer_regex = r\"Answer:\\s(.+)\"\n",
        "\n",
        "    try:\n",
        "        # find the explanation and answer in the text\n",
        "        explanation = re.search(explanation_regex, text).group(1)\n",
        "        answer = re.search(answer_regex, text).group(1)\n",
        "        return explanation, answer\n",
        "    \n",
        "    except:\n",
        "        # sometimes gpt3.5-turbo do not return the answer\n",
        "        explanation = text\n",
        "        answer = 'Answer: Not enough information.'\n",
        "        return explanation, answer"
      ],
      "metadata": {
        "id": "CvmDMnUkBB6D"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using InstructGPT (text-davinci-003)\n",
        "\n",
        "The below code defines a function `gpt3_qa` which performs the multi-document question answering task using the InstructGPT model (text-davinci-003). The function takes four arguments - `question`, `documents`, `examples`, and `get_prompt` (optional). \n",
        "\n",
        "`question` is a string that contains the question for which the answer is required.\n",
        "\n",
        "`documents` is a list of dictionaries, where each dictionary represents a document and has two keys - `title` and `content`.\n",
        "\n",
        "`examples` is a list of dictionaries, where each dictionary represents a few-shot example and has three keys - `question`, `explanation`, and `answer`. Additionally, each dictionary also has a key `documents`, which is a list of dictionaries representing the documents relevant to that example.\n",
        "\n",
        "`get_prompt` is an optional boolean argument. If set to `True`, the function returns the generated prompt instead of the extracted explanation and answer.\n",
        "\n",
        "The function appends the target question and documents to the end of the instruction prompt, and then generates the response using the InstructGPT model. The generated response is then passed through the `extract_explanation_and_answer` function to extract the explanation and answer. \n",
        "\n",
        "Finally, the function returns the extracted explanation and answer. If `get_prompt` is set to `True`, the function returns the generated prompt instead of the extracted explanation and answer."
      ],
      "metadata": {
        "id": "VrbwaRErmAbC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gpt3_qa(question, documents, examples, get_prompt=False):\n",
        "  \n",
        "  prompt = instruction # Write the instruction\n",
        "\n",
        "  # Add the few-shot examples\n",
        "  for i, example in enumerate(examples):\n",
        "    prompt += f\"Example {i+1}:\\n\"\n",
        "    for j, doc in enumerate(example[\"documents\"]):\n",
        "      prompt += f\"[Document {j+1}]: Title: {doc['title']}. Content: {doc['content']}\\n\"\n",
        "    prompt += f\"Question: {example['question']}\\nExplanation: {example['explanation']}\\nAnswer: {example['answer']}\\n##\\n\"\n",
        "  \n",
        "  # Add target example\n",
        "  prompt += f\"Example {i+2}:\\n\"\n",
        "  for k, doc in enumerate(documents):\n",
        "    prompt += f\"[Document {k+1}]: Title: {doc['title']}. Content: {doc['content']}\\n\"\n",
        "  prompt += f\"Question: {question}\"\n",
        "\n",
        "  if get_prompt: # return the formated prompt\n",
        "    return prompt\n",
        "\n",
        "  res = generate(prompt) # perform API call\n",
        "\n",
        "  return extract_explanation_and_answer(res) # post-process the output and return exaplation and answer"
      ],
      "metadata": {
        "id": "VSWsoE24rPaa"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using ChatGPT (gpt-3.5-turbo)\n",
        "\n",
        "The `chatgpt_qa` function is used to perform multi-document question answering using the ChatGPT `gpt-3.5-turbo` model. Similar to the InstructGPT function, the `chatgpt_qa` function requires a question and a list of documents as inputs. \n",
        "\n",
        "When using the `gpt-3.5-turbo` model from OpenAI API for a conversational task, it is necessary to define three roles: `system`, `user`, and `assistant`. These roles are used to create a conversation flow between the user and the model, where the `system` role represents the model, the `user` role represents the user input, and the `assistant` role represents the model's response. [See more](https://platform.openai.com/docs/guides/chat)\n",
        "\n",
        "\n",
        "To prompt the model, the function uses a list of `messages`, which includes a `\"role\"` key to specify whether the message is from the `\"user\"` or the `\"assistant\"`. The first message in the list is a system message that includes the `instruction` on how the model should answer the question. \n",
        "\n",
        "The function also appends the few-shot examples to the `messages` list, with each example being split into two messages: one from the user with the example question and documents, and one from the assistant with the example explanation and answer. \n",
        "\n",
        "Finally, the function appends a message from the user with the target question and documents to the `messages` list. The `generate_chat` function is then called to perform the API call to the OpenAI API using the `messages` list as input. \n",
        "\n",
        "The output of the `generate_chat` function is then post-processed using the `extract_explanation_and_answer` function to extract the explanation and answer provided by the model."
      ],
      "metadata": {
        "id": "nycpk4AiHfS9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  TODO\n",
        "\n",
        "# global variable (not ellegant, but effective in $$)\n",
        "\n",
        "chatGPT_output = [] # to avoid re-run chatGPT\n",
        "\n",
        "def chatgpt_qa(question, documents, examples, model=\"gpt-3.5-turbo\", get_prompt=False, verbose=True):\n",
        "    # Write the instruction\n",
        "    prompt = instruction \n",
        "\n",
        "    # Add the few-shot examples\n",
        "    for i, example in enumerate(examples):\n",
        "        prompt += f\"Example {i+1}:\\n\"\n",
        "        for j, doc in enumerate(example[\"documents\"]):\n",
        "            prompt += f\"[Document {j+1}]: Title: {doc['title']}. Content: {doc['content']}\\n\"\n",
        "        prompt += f\"Question: {example['question']}\\nExplanation: {example['explanation']}\\nAnswer: {example['answer']}\\n##\\n\"\n",
        "    \n",
        "    # Add target example\n",
        "    prompt += f\"Example {i+2}:\\n\"\n",
        "    for k, doc in enumerate(documents):\n",
        "        prompt += f\"[Document {k+1}]: Title: {doc['title']}. Content: {doc['content']}\\n\"\n",
        "    prompt += f\"Question: {question}\"\n",
        "    \n",
        "    # return the formated prompt\n",
        "    if get_prompt: \n",
        "        return prompt\n",
        "    \n",
        "    # perform API call\n",
        "    res = generate(prompt) \n",
        "\n",
        "    if verbose:\n",
        "        print(res)\n",
        "    \n",
        "    chatGPT_output.append(res)\n",
        "    # post-process the output and return exaplation and answer\n",
        "    return extract_explanation_and_answer(res) "
      ],
      "metadata": {
        "id": "yPe4KWHuV7Rj"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How much will it cost\n",
        "\n",
        "Before running our experiments, it's important to know how much it will cost to use OpenAI API. The price of the API is based on the number of tokens used. Each request to the API costs a certain number of tokens, which can vary depending on the size of the model used, the length of the prompt, and other factors.\n",
        "\n",
        "As of May 9, 2023, the costs of using OpenAI API are:\n",
        "\n",
        "* GPT-4: \\$0.03 /1k tokens\n",
        "* GPT-3.5-turbo: \\$0.002/1k tokens\n",
        "* InstructGPT (text-davinci-003): \\$0.02/1k tokens\n",
        "\n",
        "[OpenAI API Pricing](https://openai.com/pricing)\n",
        "\n",
        "The following code estimates the cost of running a few-shot question-answering experiment using OpenAI's GPT models. The estimation is based on the length of the input prompt and the expected length of the model output, which is set to 128 tokens in this case. The code uses the **`gpt3_qa`** function to generate prompts for each question in the test set, with a fixed number of examples (2 in this case) used for each question. The length of each prompt is then calculated using tokenizer loaded using **`tiktoken`** library. The final cost estimation takes into account the total length of all prompts, divided by 1000, and multiplied by the price per token of the selected model.\n",
        "\n"
      ],
      "metadata": {
        "id": "ckzQjGMJBmYD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tiktoken\n",
        "\n",
        "prompt_lengths = []\n",
        "k_shot = 3 # @param [1,2,3]\n",
        "expected_output_size = 128 # @param\n",
        "models = [\"text-davinci-003\",\"gpt-3.5-turbo\",\"gpt-4\"]\n",
        "for model in models:\n",
        "\n",
        "    model_price = {\"text-davinci-003\": 0.02, \"gpt-3.5-turbo\": 0.002, \"gpt-4\": 0.03}[model]\n",
        "\n",
        "    tokenizer = tiktoken.encoding_for_model(model)\n",
        "\n",
        "    for question in test_set_questions:\n",
        "        prompt = gpt3_qa(question['question'], question['documents'],examples[:k_shot], get_prompt=True)\n",
        "        \n",
        "        tokens = tokenizer.encode(prompt)\n",
        "\n",
        "        prompt_lengths.append(len(tokens)+expected_output_size)\n",
        "\n",
        "    price = (np.sum(prompt_lengths)/1000) * model_price\n",
        "    print(f\"This experiment will cost about U$ {price:.2f} using {model}\")"
      ],
      "metadata": {
        "id": "aKjuqbqBB31G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e970b38f-4835-4f68-daef-6fade6001150"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This experiment will cost about U$ 1.31 using text-davinci-003\n",
            "This experiment will cost about U$ 0.26 using gpt-3.5-turbo\n",
            "This experiment will cost about U$ 5.94 using gpt-4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running\n",
        "\n",
        "The code provided below is the main code to run the experiments. It uses the test questions set and loops through each question, retrieving the ground truth documents, the query question, and the pre-defined examples. Then, it uses the OpenAI API to retrieve the predicted answer and explanation to the given question using either the **`gpt3_qa`** or the **`chatgpt_qa`** function, depending on the chosen model. Afterward, it stores the predicted answer and explanation in the **`question`** dictionary. This process is repeated for each question in the test set. The **`tqdm`** library is used to display a progress bar that informs the current status of the loop. The **`k_shot`** parameter defines the number of documents the model can use to learn from.\n",
        "\n",
        "❗ **Disclaimer** ❗: The user assumes entire responsibility over the expenses on using OpenAI API by running this example. Please be aware of the API usage and cost associated with each call, and use it at your own risk. Make sure to choose the correct model and set the correct parameters to avoid unnecessary costs. Also, running the cell multiple times may result in multiple API calls and higher costs."
      ],
      "metadata": {
        "id": "Flq0oF-ZIn_t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "model = \"gpt-3.5-turbo\" # @param [\"gpt-3.5-turbo\",\"text-davinci-003\",\"gpt-4\"]\n",
        "k_shot = 3 # @param [1,2,3]\n",
        "for ix, question in enumerate(tqdm(test_set_questions)):\n",
        "    if model in [\"gpt-3.5-turbo\", \"gpt-4\"]:\n",
        "        explanation, answer = chatgpt_qa(question['question'], question['documents'], examples[:int(k_shot)])\n",
        "    else:\n",
        "        explanation, answer = gpt3_qa(question['question'], question['documents'], examples[:k_shot])\n",
        "    question[\"explanation\"] = explanation\n",
        "    question[\"predicted_answer\"] = answer"
      ],
      "metadata": {
        "id": "Ix-orTUCIpmE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b209a447-0d7b-4ab4-ac1e-652b8a194fec"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  2%|▏         | 1/52 [00:04<04:02,  4.76s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Explanation: Document 1 states that the Palici are the sons of Zeus. Document 2 states that the Palici are from Greek mythology. Document 3 states that Zeus is the sky and thunder god in ancient Greek religion. Therefore, Zeus is known for being the sky and thunder god in Greek mythology.\n",
            "Answer: being the sky and thunder god\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  4%|▍         | 2/52 [00:11<04:44,  5.70s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Explanation: Document 1 states that Messe became aide-de-camp to King Victor Emmanuel III from 1923 to 1927. Document 2 states that the First World War lasted from 28 July 1914 to 11 November 1918. Therefore, the First World War had been over for 5 years when Messe was named aide-de-camp.\n",
            "Answer: 5 years\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  6%|▌         | 3/52 [00:17<04:56,  6.06s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Explanation: Document 1 states that Messe was born in Mesagne, in the Province of Brindisi in the Apulia region of Italy on 10 December 1883. Document 2 states that the First World War started on 28 July 1914. Therefore, Messe was 30 years old when the First World War started.\n",
            "Answer: 30 years old\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  8%|▊         | 4/52 [00:25<05:30,  6.88s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Explanation: Document 1 states that Brunt returned to first-team action after eight months out with an anterior cruciate knee. Document 2 says that this return was at The Hawthorns on 15 October 2016. Document 3 states that The Hawthorns is an all-seater football stadium in West Bromwich, West Midlands, England, with a capacity of 26,688. Therefore, the capacity of the stadium where Brunt returned to action after a torn ACL is 26,688.\n",
            "Answer: 26,688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 10%|▉         | 5/52 [00:31<05:03,  6.47s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Explanation: Documents 1, 2, and 3 provide information about Chris Brunt's second goal of the 2016-17 season, but they do not provide any information about the manager of Hull City at the time.\n",
            "Therefore, there is not enough information to answer the question.\n",
            "Answer: not enough information.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 12%|█▏        | 6/52 [00:40<05:33,  7.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Explanation: Document 1 states that Chris Brunt played at The Hawthorns. Document 3 says that The Hawthorns has a capacity of 26,688. Document 2 states that Chris Brunt played at White Hart Lane. Document 4 says that White Hart Lane had a capacity of 36,284 before demolition. Therefore, White Hart Lane can hold more people than The Hawthorns.\n",
            "Answer: White Hart Lane\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 13%|█▎        | 7/52 [00:47<05:31,  7.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Explanation: Document 1 states that the album was ranked number 48 on Rolling Stone's list of the 500 Greatest Albums of All Time. However, there is not enough information to determine which albums were ranked higher than \"It Takes a Nation of Millions to Hold Us Back\".\n",
            "Answer: not enough information.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 15%|█▌        | 8/52 [00:57<05:52,  8.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Explanation: Document 1 states that the Turks and Caicos Islands became an affiliate member of the International Cricket Council (ICC) in 2002. Document 2 states that the ICC was founded as the Imperial Cricket Conference in 1909. Therefore, the sports organization that the Turks and Caicos Islands became affiliate members in 2002 was founded in 1909.\n",
            "Answer: 1909\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 17%|█▋        | 9/52 [01:03<05:25,  7.56s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Explanation: Document 1 states that the Turks and Caicos Islands played the Bahamas in the Americas Affiliates Championship in 2004. However, there is not enough information to determine the official language of the country.\n",
            "Answer: not enough information.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 19%|█▉        | 10/52 [01:11<05:22,  7.67s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Explanation: Document 1 states that the Turks and Caicos Islands national cricket team finished as runners up in Division Three of the ICC Americas Championship in 2006. Document 2 states that the ICC Americas Championship first took place in 2000. Therefore, the founding date of the championship the Turks and Caicos Islands national cricket team finished as runners up in 2006 was 2000.\n",
            "Answer: 2000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 21%|██        | 11/52 [01:22<05:54,  8.65s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Explanation: Document 1 states that the Turks and Caicos Islands national cricket team were invited to take part in the 2008 Standford 20/20, playing one match in a preliminary round against Montserrat. Document 2 says that Donovan Matthews top-scored for the team with 25. Document 3 states that Matthews played a single Twenty20 match for the Turks and Caicos Islands against Montserrat in the 2008 Stanford 20/20 at the Stanford Cricket Ground. Therefore, Donovan Matthews only played for the Turks and Caicos Islands for one year during the 2008 Standford 20/20.\n",
            "Answer: 1 year\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 23%|██▎       | 12/52 [01:28<05:14,  7.87s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Explanation: Document 1 states that Nightmare was released on July 27, 2010 and was produced by Mike Elizondo. However, there is not enough information to determine how many years Mike Elizondo had been a producer when Nightmare was released.\n",
            "Answer: not enough information.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 25%|██▌       | 13/52 [01:34<04:40,  7.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Explanation: Document 1 and 2 both state that the album was mixed in New York City by noted engineer Andy Wallace. Document 3 states that the present mayor of New York City is Bill de Blasio. Therefore, the mayor of the city where the album was mixed by Andy Wallace is Bill de Blasio.\n",
            "Answer: Bill de Blasio\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 27%|██▋       | 14/52 [01:41<04:35,  7.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Explanation: Document 1 states that Baghdad Jewish Arabic is the Arabic dialect spoken by the Jews of Baghdad. Document 2 states that Baghdad is the capital of Iraq. Therefore, the city where the Arabic dialect is spoken by Jews is located in Iraq.\n",
            "Answer: Iraq\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 29%|██▉       | 15/52 [01:52<05:03,  8.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Explanation: Document 1 states that the archaeology of the Maya site of Cerén has been ongoing since its discovery in 1978. Document 2 says that the site was discovered in 1976. Document 3 states that Pompeii was destroyed in AD 79. Therefore, there were 97 years between Pompeii's destruction and the discovery of the Maya site of Cerén.\n",
            "Answer: 97 years\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 31%|███       | 16/52 [01:59<04:46,  7.95s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Explanation: Document 1 and 2 state that Payson D. Sheets has done extensive work at Cerén in El Salvador and Arenal in Costa Rica. Document 3 and 4 provide information about the size of El Salvador and Costa Rica. El Salvador has a total area of 21,041 km2 while Costa Rica has a total area of 51100 km2. Therefore, Costa Rica is the larger of the two countries.\n",
            "Answer: Costa Rica\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 33%|███▎      | 17/52 [02:06<04:28,  7.67s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Explanation: Document 1 states that Christopher Harison first arrived in the Cape of Good Hope in 1849 as a captain in the Perthshire Regiment. Document 2 states that the 73rd Regiment of Foot was raised in 1780. Therefore, the Perthshire Regiment had been a going concern for 69 years when Harison joined it as a captain.\n",
            "\n",
            "Answer: 69 years\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 35%|███▍      | 18/52 [02:14<04:26,  7.85s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Explanation: Document 1 states that Christopher Harison transferred to Tokai in Cape Town as Conservator over the Western Conservancy in 1888. Document 2 states that Harison transferred to Tokai in 1888. However, neither document mentions the population of Cape Town at the time.\n",
            "Answer: Not enough information.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 37%|███▋      | 19/52 [02:25<04:45,  8.64s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Explanation: Document 1 states that the 2004 regular season had ended. Document 2 says that La Russa's Cardinals defeated the Los Angeles Dodgers in the National League Division Series, 3 games to 1. Document 3 mentions that Jason Isringhausen closed out the game after allowing a home run to Tom Wilson in the ninth. This suggests that the Cardinals won the first game in the 2004 National League Division Series.\n",
            "Answer: The Cardinals\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 38%|███▊      | 20/52 [02:30<04:04,  7.65s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Explanation: Document 1 states that St. Louis took on the Houston Astros in the National League Championship Series. However, there is not enough information to determine which team scored more runs total in the National League Championship Series in 2004.\n",
            "Answer: not enough information.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 21/52 [02:33<03:16,  6.35s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Explanation: There is not enough information in the document to answer this question.\n",
            "Answer: Not enough information.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 42%|████▏     | 22/52 [02:40<03:16,  6.55s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Explanation: Document 1 states that the landmark concert was held at the Grand Olympic Auditorium on April 13, 1984. Document 2 says that the venue was built in 1924. Therefore, the Grand Olympic Auditorium was 60 years old at the time of New Regime playing a landmark concert there.\n",
            "Answer: 60 years old\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 44%|████▍     | 23/52 [02:46<03:04,  6.36s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Explanation: The document does not provide any information about the number of records sold by either MDC or UK Subhumans. Therefore, there is not enough information to answer the question.\n",
            "Answer: not enough information.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 46%|████▌     | 24/52 [02:52<02:49,  6.05s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Explanation: Document 1 states that the Damodar River is the most important river of the Chota Nagpur Plateau and that it flows along the southern border. However, there is not enough information to determine the length of the river.\n",
            "Answer: not enough information.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 48%|████▊     | 25/52 [03:05<03:42,  8.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Explanation: Document 1 states that Jordan signed a minor league baseball contract with the Chicago White Sox and was assigned to the team's minor league system on March 31, 1994. Document 2 states that he had an unspectacular professional baseball career for the Birmingham Barons, a Chicago White Sox farm team. Document 3 says that the White Sox were established in 1900. Document 4 states that the current team of the Birmingham Barons arrived in the Birmingham area in 1981. Therefore, the Chicago White Sox had been around longer than the Birmingham Barons when he was assigned to the White Sox's minor league system.\n",
            "Answer: yes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|█████     | 26/52 [03:12<03:21,  7.75s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Explanation: Document 1 states that Bulls owner Jerry Reinsdorf. Document 2 states that Jerry M. Reinsdorf is an owner of the NBA's Chicago Bulls and the MLB's Chicago White Sox. This suggests that Jerry Reinsdorf is the owner of both the Bulls and the White Sox.\n",
            "Answer: yes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 52%|█████▏    | 27/52 [03:22<03:33,  8.54s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Explanation: Document 1 mentions that Jordan signed a minor league baseball contract with the Chicago White Sox. Document 2 states that the Chicago White Sox play in New Comiskey Park (redubbed U.S. Cellular in 2003 and Guaranteed Rate Field in 2016). Therefore, the Chicago White Sox play in Guaranteed Rate Field.\n",
            "Answer: Guaranteed Rate Field\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 54%|█████▍    | 28/52 [03:28<03:04,  7.71s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Explanation: Document 1 states that Gilmore retired in 2006. Document 2 states that Eduard van Beinum died in 1959. Therefore, Eduard van Beinum was not alive when Gilmore retired in 2006.\n",
            "Answer: no\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 56%|█████▌    | 29/52 [03:36<03:00,  7.86s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Explanation: Document 1 states that Gilmore taught at Cornell University and Oregon State University. Document 2 states that he taught at Cornell University and Oregon State University before joining the faculty of UCI in 1982. However, there is not enough information to determine which of the schools had more students.\n",
            "Answer: Not enough information.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 58%|█████▊    | 30/52 [03:44<02:54,  7.95s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Explanation: Document 1 states that Lewis was born on Bothwick plantation, Dinwiddie County, Virginia. Document 2 says that he constructed a house (\"Old Homestead\") in the town of Lowndesboro, Alabama. However, there is not enough information to determine how far away the house was from Lewis' birthplace.\n",
            "Answer: not enough information.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|█████▉    | 31/52 [03:57<03:19,  9.48s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Explanation: Document 1 states that Sherman returned to serve under General Ulysses S. Grant in the winter of 1862 during the battles of forts Henry and Donelson. Document 2 says that thirty-two crewmen were killed or wounded, including commander William D. Porter, in the Battle of Fort Henry. Document 3 states that nearly 1,000 soldiers on both sides had been killed in the Battle of Fort Donelson. Therefore, the Battle of Fort Donelson had a higher Union casualty rate than the Battle of Fort Henry.\n",
            "Answer: Battle of Fort Donelson\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 62%|██████▏   | 32/52 [04:04<02:56,  8.80s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Explanation: Document 1 states that those who escaped deportation were dispersed across northern Mexico and some even settled across the international border in southern Arizona. Document 2 states that Arizona is a state in the southwestern region of the United States. Therefore, some of the tribes escaped to the United States after their deportation from northern Mexico.\n",
            "Answer: United States\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 63%|██████▎   | 33/52 [04:13<02:43,  8.60s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Explanation: Document 1 mentions Edward C. Tolman and Robert Tryon. Document 2 states that Hirsch began his interest in behavior genetics in the 1950s, as a student at the University of California, Berkeley, where he studied under Edward C. Tolman and Robert Tryon. Document 3 says that the University of California, Berkeley is located in the United States. Therefore, Hirsch studied under Edward C. Tolman and Robert Tryon in the United States.\n",
            "Answer: United States\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 65%|██████▌   | 34/52 [04:17<02:11,  7.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Explanation: Document 1 states that Jerry Hirsch was an assistant professor at Columbia University from 1956 to 1960. Document 2 states that Columbia University is a private Ivy League research university in New York City. Therefore, Hirsch worked as an assistant professor in New York from 1956 to 1960.\n",
            "Answer: New York\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 67%|██████▋   | 35/52 [04:27<02:18,  8.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Explanation: Document 1 states that in 1913 Marcel Duchamp used Palmer's 1910 photograph of the illuminated Grand Pier Pavilion as found object art in his Note 78. Document 2 states that Marcel Duchamp was born at Blainville-Crevon in Normandy, France. Therefore, the birthplace of the person who used Palmer's 1910 photograph of the illuminated Grand Pier Pavilion as found object art in his Note 78 is Blainville-Crevon in Normandy, France.\n",
            "Answer: Blainville-Crevon in Normandy, France.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 69%|██████▉   | 36/52 [04:30<01:47,  6.69s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Explanation: Document 1 states that Yuri Tschinkel graduated from the Lomonosov Moscow State University in 1990. However, there is not enough information in the document to determine how many students were enrolled at the school the year Yuri Tschinkel graduated.\n",
            "Answer: not enough information.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 71%|███████   | 37/52 [04:34<01:25,  5.68s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Explanation: There is not enough information in the document to answer this question.\n",
            "Answer: Not enough information.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 73%|███████▎  | 38/52 [04:38<01:12,  5.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Explanation: Document 1 states that Tschinkel was a junior fellow at Harvard University from 1992 to 1995. However, there is not enough information in the document to determine the budget of the Simons Foundation the year Tschinkel became a junior fellow at Harvard University.\n",
            "Answer: not enough information.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 75%|███████▌  | 39/52 [04:43<01:08,  5.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Explanation: Despite the document 1 being about Yuri Tschinkel and saying when he passed the Abitur, there is not enough information to determine the population of East Berlin at that time.\n",
            "Answer: not enough information.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 77%|███████▋  | 40/52 [04:46<00:55,  4.62s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Explanation: Document 3 states that Gorbachev was the country's head of state from 1985 until 1991. Therefore, he had been in power for 6 years before the attempted coup.\n",
            "Answer: 6 years\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 79%|███████▉  | 41/52 [04:54<00:59,  5.43s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Explanation: Document 1 is about the 106th Guards Airborne Division and mentions the attempted coup against the Soviet President Mikhail Gorbachev in Moscow. However, there is not enough information to determine who was in command of the Tamanskaya Divison during the failed coup.\n",
            "Answer: not enough information.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 81%|████████  | 42/52 [04:58<00:52,  5.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Explanation: Document 1 states that Romeo Miller was with his father Master P in 2005. Document 2 states that Percy Robert Miller, Romeo's father, was born and raised in New Orleans. Therefore, Romeo's father was born in New Orleans.\n",
            "Answer: New Orleans\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 83%|████████▎ | 43/52 [05:08<00:59,  6.64s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Explanation: Document 1 states that Julian Baretta won a Centennial Cup in 1975 with the Spruce Grove Mets. Document 2 states that the team moved to the suburban city of Spruce Grove to become the Spruce Grove Mets as of the 1974–75 season. Therefore, the Spruce Grove Mets had been a team for one season when they won the Centennial Cup in 1975.\n",
            "Document 3 states that Julian Baretta won a Centennial Cup in 1975. This supports the conclusion that the Spruce Grove Mets had been a team for one season when they won the Centennial Cup.\n",
            "Answer: 1 season\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 85%|████████▍ | 44/52 [05:13<00:47,  5.96s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Explanation: Document 1 states that Swinburne first played county cricket for Devon in the 1964. However, there is not enough information to determine which team won the championship in that year.\n",
            "Answer: not enough information.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 87%|████████▋ | 45/52 [05:18<00:39,  5.71s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Explanation: Despite the document being about John Swinburne and his first-class debut in the County Championship, there is not enough information to determine which team won the County Championship in 1970.\n",
            "Answer: not enough information.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 88%|████████▊ | 46/52 [05:23<00:32,  5.50s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Explanation: Document 1 states that Hurricane Barry was the second named storm of the annual hurricane season. Document 2 states that Subtropical Storm Andrea was the first named storm of the 2019 hurricane season.\n",
            "Answer: Subtropical Storm Andrea\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 90%|█████████ | 47/52 [05:27<00:25,  5.07s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Explanation: Document 1 states that the Cardinals moved into the Cardinals Stadium in Glendale, Arizona. Document 2 states that the cost of the project was $455 million. Therefore, the construction cost of the stadium that the Cardinals moved into in Glendale, Arizona was $455 million.\n",
            "Answer: $455 million\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 92%|█████████▏| 48/52 [05:32<00:20,  5.02s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Explanation: Document 1 states that the stadium was christened University of Phoenix Stadium on September 26. However, there is not enough information to determine how many people attend the university for which the Cardinals stadium is named.\n",
            "Answer: not enough information.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 94%|█████████▍| 49/52 [05:36<00:14,  4.81s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Explanation: Document 1 states that Dennis Green was fired after the season and replaced by Ken Whisenhunt. Document 2 states that Green coached the Minnesota Vikings for 10 seasons. Therefore, Dennis Green has only coached the Minnesota Vikings in the NFL.\n",
            "Answer: Minnesota Vikings\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 96%|█████████▌| 50/52 [05:44<00:11,  5.67s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Explanation: Document 1 mentions the Louisiana Recovery Authority (LRA), which was created to lead the state's rebuilding efforts following the catastrophic 2005 Hurricanes Katrina and Rita. Document 2 states that Hurricane Katrina was a Category 5 hurricane that made landfall on Florida and Louisiana in August 2005. Document 3 says that Rita formed near The Bahamas from a tropical wave on September 18, 2005. Therefore, Hurricane Katrina and Hurricane Rita hit in August and September of 2005, respectively.\n",
            "Answer: August and September of 2005\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 98%|█████████▊| 51/52 [05:53<00:06,  6.91s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Explanation: Document 1 mentions that Calvin Mackie was featured on HBO as a commentator on Spike Lee's documentary on the Katrina disaster When The Levees Broke: A Requiem in Four Parts. Document 2 states that When the Levees Broke: A Requiem in Four Acts is a 2006 documentary film directed by Spike Lee about the devastation of New Orleans, Louisiana following the failure of the levees during Hurricane Katrina. It was filmed in late August and early September 2005, and premiered at the New Orleans Arena on August 16, 2006. Therefore, Spike Lee debuted his documentary When the Levees Broke;\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 52/52 [05:55<00:00,  6.84s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Explanation: There is not enough information in the document to answer this question.\n",
            "Answer: Not enough information.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation\n",
        "\n",
        "In this section, we evaluate the performance of our model by calculating the exact match score and the F1 score using bag of words. To accomplish this, we have defined some helper functions. \n",
        "\n",
        "The **`normalize_text`** function takes a text and normalizes it by converting it to lowercase and removing any non-alphanumeric characters. The **`get_tokens`** function tokenizes the text after normalization.\n",
        "\n",
        "The **`exact_match`** function takes the predicted answer and the true answer and returns whether they match exactly after normalization. The **`f1_bag_of_words`** function takes the predicted answer and the true answer, tokenizes them, and calculates their F1 score using the bag of words approach. \n",
        "\n",
        "The bag of words approach is a technique used to measure the similarity between two sets of texts by counting the frequency of each word in both sets and then calculating their overlap."
      ],
      "metadata": {
        "id": "VCAuOALVLETs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "def normalize_text(text):\n",
        "    \"\"\"\n",
        "    Helper function to normalize the text\n",
        "    \"\"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"[^a-zA-Z0-9]+\", \" \", text)\n",
        "    return text.strip()\n",
        "\n",
        "def get_tokens(text):\n",
        "    \"\"\"\n",
        "    Helper function to tokenize text\n",
        "    \"\"\"\n",
        "    text = normalize_text(text)\n",
        "    return text.split()\n",
        "\n",
        "def exact_match(pred_answer, true_answer):\n",
        "    \"\"\"\n",
        "    Calculates the exact match score\n",
        "    \"\"\"\n",
        "    return normalize_text(pred_answer) == normalize_text(true_answer)\n",
        "\n",
        "def f1_bag_of_words(pred_answer, true_answer):\n",
        "    \"\"\"\n",
        "    Calculates the F1 score using bag of words\n",
        "    \"\"\"\n",
        "    pred_tokens = get_tokens(pred_answer)\n",
        "    true_tokens = get_tokens(true_answer)\n",
        "\n",
        "    pred_counter = Counter(pred_tokens)\n",
        "    true_counter = Counter(true_tokens)\n",
        "\n",
        "    common = pred_counter & true_counter\n",
        "    num_same = sum(common.values())\n",
        "\n",
        "    if num_same == 0:\n",
        "        return 0\n",
        "\n",
        "    precision = 1.0 * num_same / len(pred_tokens)\n",
        "    recall = 1.0 * num_same / len(true_tokens)\n",
        "    f1 = (2 * precision * recall) / (precision + recall)\n",
        "\n",
        "    return f1"
      ],
      "metadata": {
        "id": "duttlxnNQGLg"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the below code, we are evaluating the performance of the model for multi-document question answering by calculating the: Exact Match (EM) and F1 score using bag of words. \n",
        "\n",
        "The code iterates over each item in the validation dataset and calculates the F1 and EM scores using the **`f1_bag_of_words`** and **`exact_match`** functions defined earlier. The maximum score of F1 and EM is then taken for each item and appended to their respective lists, **`f1s`** and **`ems`**. \n",
        "\n",
        "The mean of the **`f1s`** and **`ems`** lists are then calculated using NumPy's **`np.mean`** function and assigned to **`mean_f1`** and **`mean_em`** variables, respectively. Finally, the average EM and F1 scores are printed using formatted string literals."
      ],
      "metadata": {
        "id": "i89xwhKNMhME"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "f1s, ems = [], []\n",
        "for question in test_set_questions:\n",
        "  if \"predicted_answer\" in question:\n",
        "    f1 = f1_bag_of_words(question[\"predicted_answer\"],question[\"answer\"])\n",
        "    em = exact_match(question[\"predicted_answer\"],question[\"answer\"])\n",
        "    f1s.append(f1)\n",
        "    ems.append(em)\n",
        "\n",
        "mean_em = np.mean(ems)\n",
        "mean_f1 = np.mean(f1s)\n",
        "print(f\"Exact match: {mean_em:.3f}\\nF1-bow: {mean_f1:.3f}\")"
      ],
      "metadata": {
        "id": "G7hrJ86CXQ62",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d492e760-7a8b-467e-b36f-83e637f8ce1f"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exact match: 0.788\n",
            "F1-bow: 0.908\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Try your own questions\n",
        "\n",
        "You can try multi-document QA using your own examples. Google for topics of your interest and provide the documents and the question as in the example below.\n",
        "\n",
        "The documents used as an example below are the first paragraph from the pages \"Pele\" and \"Steve Jobs\" from Wikipedia. It is just a toy example. You can try more realistic ones."
      ],
      "metadata": {
        "id": "BLamip1GtBcI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documents = [\n",
        "    {\n",
        "        \"title\": \"Pele\",\n",
        "        \"content\": \"Edson Arantes do Nascimento (Brazilian Portuguese: [ˈɛdsõ aˈɾɐ̃tʃiz du nasiˈmẽtu]; 23 October 1940 – 29 December 2022), better known by his nickname Pelé (Portuguese pronunciation: [peˈlɛ]), was a Brazilian professional footballer who played as a forward. Widely regarded as one of the greatest players of all time, he was among the most successful and popular sports figures of the 20th century.[2][3] In 1999, he was named Athlete of the Century by the International Olympic Committee and was included in the Time list of the 100 most important people of the 20th century. In 2000, Pelé was voted World Player of the Century by the International Federation of Football History & Statistics (IFFHS) and was one of the two joint winners of the FIFA Player of the Century. His 1,279 goals in 1,363 games, which includes friendlies, is recognised as a Guinness World Record.[4]\"\n",
        "    },\n",
        "    {\n",
        "        \"title\": \"Steve Jobs\",\n",
        "        \"content\": \"Steven Paul Jobs (February 24, 1955 – October 5, 2011) was an American business magnate, inventor, and investor. He was the co-founder, chairman, and CEO of Apple; the chairman and majority shareholder of Pixar; a member of The Walt Disney Company's board of directors following its acquisition of Pixar; and the founder, chairman, and CEO of NeXT. He was a pioneer of the personal computer revolution of the 1970s and 1980s, along with his early business partner and fellow Apple co-founder Steve Wozniak.\"\n",
        "    }\n",
        "]\n",
        "\n",
        "question = \"How old were Pelé when Steve Jobs died?\"\n",
        "\n",
        "explanation, answer = chatgpt_qa(question, documents, examples)\n",
        "\n",
        "print(f\"Explanation: {explanation}\\nAnswer: {answer}\")"
      ],
      "metadata": {
        "id": "VgD08TKztFPp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d96699c0-3600-44e4-aa11-b316371071f5"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Explanation: Document 1 states that Pelé was born on October 23, 1940. Document 2 states that Steve Jobs died on October 5, 2011. Therefore, Pelé was 70 years old when Steve Jobs died.\n",
            "Answer: 70 years old\n",
            "Explanation: Document 1 states that Pelé was born on October 23, 1940. Document 2 states that Steve Jobs died on October 5, 2011. Therefore, Pelé was 70 years old when Steve Jobs died.\n",
            "Answer: 70 years old\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cHdPTsyrW4g9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}