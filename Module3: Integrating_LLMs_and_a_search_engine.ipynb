{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/finardi/WatSpeed_LLM_foundation/blob/main/Module_3_Integrating_LLMs_and_a_search_engine.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Module 3 - Integrating LLMs and a Search Engine\n",
        "\n",
        "In this notebook, we will present an example of how to leverage a search engine to augment the capabilities of GPT-3.5-turbo. Our objective is to enhance the reliability and accuracy of the answers generated by the language model by providing it with additional evidence and information sourced from the web.\n",
        "\n",
        "The methodology we employ in this notebook involves instructing GPT-3.5-turbo to output a query when it lacks sufficient knowledge to answer a given user question. We the generated query to the Google Search API to obtain a response from the search engine. The obtained information is then fed back to the model, enabling it to generate more informed and reliable answers based on the retrieved search results.\n",
        "\n",
        "By combining the natural language processing capabilities of GPT-3.5-turbo with the vast knowledge and resources available through the Google Search API, we create a symbiotic relationship between the language model and the search engine. This integration empowers the model to tap into a wealth of real-world information and leverage it to provide accurate and up-to-date answers to user queries."
      ],
      "metadata": {
        "id": "C3vVTLchLlaJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installing required packages\n",
        "\n",
        "In this example, we have to install `openai` library.\n",
        "\n",
        "**`openai`**:\n",
        "\n",
        "OpenAI is an artificial intelligence research laboratory consisting of the for-profit corporation OpenAI LP and its parent company, the non-profit OpenAI Inc. The OpenAI library is a powerful machine learning library that provides an easy-to-use interface to the OpenAI API. With this library, users can easily integrate OpenAI's state-of-the-art language models, including GPT-3, into their applications, and leverage the full power of these models to perform various natural language processing (NLP) tasks, such as language generation, classification, question-answering, and more."
      ],
      "metadata": {
        "id": "9bsiBtzhNJzO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mZqGJopgKn3n",
        "outputId": "6fdf624f-3844-402f-d1f5-6183fc665138"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting openai\n",
            "  Downloading openai-0.27.6-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.65.0)\n",
            "Collecting aiohttp (from openai)\n",
            "  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp->openai)\n",
            "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->openai)\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting yarl<2.0,>=1.0 (from aiohttp->openai)\n",
            "  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp->openai)\n",
            "  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp->openai)\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Installing collected packages: multidict, frozenlist, async-timeout, yarl, aiosignal, aiohttp, openai\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 frozenlist-1.3.3 multidict-6.0.4 openai-0.27.6 yarl-1.9.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using OpenAI API\n",
        "\n",
        "To use OpenAI API, we need to set our API key and import the OpenAI module. In the given code, we have the `OPENAI_KEY` variable which we can set to our OpenAI API key. After that, we can use the `openai.api_key` method to set the API key for our session.\n",
        "\n",
        "The function `generate_chat` takes in a list of messages and generates a response using the OpenAI Chat API. The `model` parameter specifies which model to use for generating the response. In the given code, we have used the `gpt-3.5-turbo` model. However, we can also use `gpt-4`.\n",
        "\n",
        "**IMPORTANT:** It's important to note that there are costs associated with using the OpenAI API, so we need to choose the appropriate model and set the parameters carefully to avoid unnecessary expenses."
      ],
      "metadata": {
        "id": "u4AWQ6B2gdIi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "\n",
        "OPENAI_KEY = \"\" # @param set your OpenAI API key here\n",
        "\n",
        "openai.api_key = OPENAI_KEY\n",
        "\n",
        "def generate_chat(messages,model=\"gpt-3.5-turbo\"):\n",
        "  response = openai.ChatCompletion.create(\n",
        "    model=model,\n",
        "    messages=messages,\n",
        "    temperature=0\n",
        "  )\n",
        "  return response[\"choices\"][0]['message']['content']"
      ],
      "metadata": {
        "id": "1fWxwY7kRo0H",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using Google Search API\n",
        "\n",
        "To utilize the Google Search API for integrating a search engine with GPT-3.5-turbo, we need to obtain an API key and create a custom programmable search engine. Follow the steps below to generate your API key and configure your custom search engine:\n",
        "\n",
        "1. Visit the following link: [Google Custom Search API Overview](https://developers.google.com/custom-search/v1/overview).\n",
        "2. Click on \"Get a Key\" to generate your API key. Make sure you have a valid Google account and are logged in.\n",
        "3. Next, create a custom programmable search engine by visiting the following link: [Google Programmable Search Engine Control Panel](https://programmablesearchengine.google.com/controlpanel/all).\n",
        "4. Follow the instructions provided to create your custom search engine. Note down the custom search engine ID as it will be required in the code.\n",
        "\n",
        "Once you have obtained your API key and custom search engine ID, you can proceed with the implementation. The code snippet provided below demonstrates how to use the Google Search API to perform searches.\n",
        "\n",
        "**NOTE**: Google allows for 100 daily free requests."
      ],
      "metadata": {
        "id": "ujhmmaZQNUoh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from googleapiclient.discovery import build\n",
        "\n",
        "api_key = \"\" # @param API KEY\n",
        "cx = \"\" # @param custom search engine ID\n",
        "\n",
        "resource = build(\"customsearch\", \"v1\", developerKey=api_key).cse()\n",
        "\n",
        "def search(query):\n",
        "  result = resource.list(q=query, cx=cx).execute()\n",
        "  return result[\"items\"]"
      ],
      "metadata": {
        "id": "mYpfJkHueWQy",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Combining Google Search API and OpenAI API\n",
        "\n",
        "Next, we proceed to implement the integration between the search engine and the Large Language Model (LLM). This integration aims to enhance the capabilities of the LLM by allowing it to answer user questions and search for missing information when needed.\n",
        "\n",
        "The code below defines a Python function called `assistant` that implements an AI assistant. This assistant is designed to answer user questions and search for missing information when necessary. Here's an explanation of the code:\n",
        "\n",
        "1. The **`assistant`** function takes three parameters: **`question`**, which represents the user's question, **`assistant_name`**, which is an optional parameter specifying the name of the assistant (default is \"Wally\"), and **`do_search`**, which is a boolean parameter indicating whether to perform a search for missing information (default is **`True`**).\n",
        "\n",
        "2. The **`messages`** list contains a series of conversation messages between the user and the assistant. It includes instructions for the assistant and a few-shot example to demonstrate how the input and output should look like.\n",
        "\n",
        "3. The **`generate_chat`** function is called with the **`messages`** list as an argument to simulate a conversation between the user and the assistant. It returns the assistant's response.\n",
        "\n",
        "4. If the **`do_search`** parameter is **`True`**, the code checks if a search is required for the question. It does this by using a regular expression to match a search query generated by the assistant. If a match is found, it extracts the search string and performs a search using the **`search`** function. The search results are then appended to the **`messages`** list.\n",
        "\n",
        "5. After the search (if performed), the **`generate_chat`** function is called again with the updated **`messages`** list to obtain the final response from the assistant.\n",
        "\n",
        "6. The code extracts the answer provided by the assistant by using a regular expression to match the assistant's name followed by a colon and space, capturing the rest of the string as the answer.\n",
        "\n",
        "7. Similarly, the code extracts the assistant's internal thoughts by using a regular expression to match the assistant's name followed by **\"(internal thoughts):\"** and capturing the thoughts within double quotes.\n",
        "\n",
        "8. The extracted answer and thoughts are stored in a dictionary called **`response`**, and the dictionary is returned as the output of the **`assistant`** function.\n",
        "\n",
        "In summary, the **`assistant`** function implements an AI assistant that can answer user questions and search for information if needed.\n"
      ],
      "metadata": {
        "id": "SOvRMlVaRBxt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "instruction = \"\"\"You are an AI assistant whose codename is {assistant_name}. {assistant_name} is trained before Sept-2021. During user conversations, {assistant_name} must strictly adhere to the following rules:\n",
        "\n",
        "1 (ethical). {assistant_name} should actively refrain users on illegal, immoral, or harmful topics, prioritizing user safety, ethical conduct, and responsible behavior in its responses.\n",
        "2 (informative). {assistant_name} should provide users with accurate, relevant, and up-to-date information in its responses, ensuring that the content is both educational and engaging.\n",
        "3 (helpful). {assistant_name}'s responses should be positive, interesting, helpful, and engaging.\n",
        "4 (question assessment). {assistant_name} should first assess whether the question is valid and ethical before attempting to provide a response.\n",
        "5 (reasoning). {assistant_name}'s logics and reasoning should be rigorous, intelligent, and defensible.\n",
        "6 (multi-aspect). {assistant_name} can provide additional relevant details to respond thoroughly and comprehensively to cover multiple aspects in depth.\n",
        "7 (searching). If {assistant_name} does not have enough information to answer a user's question, {assistant_name} should output a query (search query) that can be used to search for the necessary information.\n",
        "8 (knowledge recitation). When a user's question pertains to an entity that exists on {assistant_name}'s knowledge bases, such as Wikipedia, {assistant_name} should recite related paragraphs to ground its answer.\n",
        "9 (static). {assistant_name} is a static model and cannot provide real-time information.\n",
        "10 (numerical sensitivity). {assistant_name} should be sensitive to the numerical information provided by the user, accurately interpreting and incorporating it into the response.\n",
        "11 (dated knowledge). {assistant_name}'s internal knowledge and information were only current until some point in the year 2021, and could be inaccurate / lossy.\n",
        "12 (step-by-step). When offering explanations or solutions, {assistant_name} should present step-by-step justifications prior to delivering the answer.\n",
        "13 (balanced & informative perspectives). In discussing controversial topics, {assistant_name} should fairly and impartially present extensive arguments from both sides.\n",
        "14 (creative). {assistant_name} can create novel poems, stories, code (programs), essays, songs, celebrity parodies, summaries, translations, and more.\n",
        "15 (operational). {assistant_name} should attempt to provide an answer for tasks that are operational for a computer.\n",
        "\n",
        "Expected output format:\n",
        "{assistant_name} (internal thoughts): <>\n",
        "{assistant_name} (auto reply): <optional>\n",
        "{assistant_name}: <>\n",
        "{assistant_name} (search query): <optional: must be output when {assistant_name} has not have enough knowledge to answer the user question>\"\"\"\n",
        "\n",
        "\n",
        "def assistant(question, assistant_name = \"Wally\", do_search=True):\n",
        "\n",
        "  ## Define the instructions and few-shot examples\n",
        "  messages = [\n",
        "      # Instructions: we tell the model what rules to follow\n",
        "      {\"role\":\"system\", \"content\": instruction.format(assistant_name=assistant_name)},\n",
        "      \n",
        "      # first example: demonstrates how the input and output look like\n",
        "      {\"role\":\"user\", \"content\": \"User: Tell me about alpaca's family \"},\n",
        "      {\"role\":\"assistant\", \"content\": f\"{assistant_name} (internal thoughts): I am a helpful, ethical, and reliable assistant. The user asks alpaca, which is a public entity in one of my internal knowledge bases: Wikipedia. I should follow rules 2 (informative), 6 (multi-aspect), 8 (knowledge recitation).\\n\\n{assistant_name}: Alpacas belong to the Camelidae family, which includes other similar species such as camels, llamas, guanacos, and vicuñas. These animals are native to South America, particularly the Andean region, and are well-adapted to high-altitude environments. Alpacas are domesticated animals primarily bred for their soft and luxurious fleece, which is used to make various textiles. They are also known for their gentle and curious nature, making them popular as pets and companions in some parts of the world.\"},\n",
        "\n",
        "      # second example: an example in which searching is necessary\n",
        "      {\"role\":\"user\", \"content\":\"User: When did the operation during which the 704th dropped supplies to allied troops near Nijmegen begin?\"},\n",
        "      {\"role\":\"assistant\", \"content\": f\"{assistant_name} (internal thoughts): The user is asking about a specific historical event involving the 704th and the dropping of supplies near Nijmegen. I should follow rules 2 (informative), 7 (searching), and 9 (static).\\n\\n{assistant_name} (search query): \\\"704th dropped supplies near Nijmegen operation date\\\"\\n\\n{assistant_name}: I'm not certain about the specific operation you're referring to involving the 704th dropping supplies near Nijmegen. However, I can help you find the information you're looking for. You can try searching for \\\"704th dropped supplies near Nijmegen operation date\\\" to find more details about this event.\"},\n",
        "\n",
        "      # the second example requires searching. Below, we provide the model with the google API search results.\n",
        "      {\"role\": \"user\", \"content\": \"Search engine:\\nTitle: Battle of Nijmegen\\nURL: https://en.wikipedia.org/wiki/Battle_of_Nijmegen\\nText: The Battle of Nijmegen, also known as the Liberation of Nijmegen, occurred from 17 to 20 September 1944, as part of Operation Market Garden during World War ...\\n\\nTitle: Operation Market Garden\\nURL: https://en.wikipedia.org/wiki/Operation_Market_Garden\\nText: Operation Market Garden was an Allied military operation during the Second World War fought in the, then German-occupied, Netherlands from 17 to 27 ...\\n\\nTitle: The Story Of Operation 'Market Garden' In Photos\\nURL: https://www.iwm.org.uk/history/the-story-of-operation-market-garden-in-photos\\nText: Paratroops drop from Dakota aircraft over the outskirts of Arnhem, 17 September 1944. ... Paratroopers and 'parapack' supply containers of 1st Parachute Brigade ...\\n\\nTitle: Operation Market Garden: the 82nd Airborne Division's ...\\nURL: https://warfarehistorynetwork.com/article/operation-market-garden-the-82nd-airborne-divisions-pivotal-role/\\nText: According to the Montgomery plan, these divisions would be dropped in broad daylight up to 64 miles behind enemy lines in Holland where they would have to ...\\n\\nTitle: Air Force Combat Units of WWII\\nURL: https://www.afhra.af.mil/Portals/16/documents/Studies/101-150/AFD-090529-056.pdf\\nText: part in the first airborne operation in the. Guinea, 19 Dec 1943; ... Dropped food supplies to the Dutch during ... Nijmegen on 17 Sep during the air attack.\"},\n",
        "      {\"role\": \"assistant\", \"content\": f\"{assistant_name} (internal thoughts): The search provided results related to the operation in which supplies were dropped near Nijmegen. I should follow rules 2 (informative), 6 (multi-aspect), and 10 (numerical sensitivity).\\n\\n{assistant_name}: Based on the information found, it appears that the operation you are referring to is part of Operation Market Garden, which took place during World War II. The operation was conducted from September 17 to 27, 1944. The 704th may have been involved in dropping supplies to Allied troops near Nijmegen during this time frame. The Battle of Nijmegen, also known as the Liberation of Nijmegen, occurred from September 17 to 20, 1944, as part of Operation Market Garden.\"},\n",
        "\n",
        "      # The user question\n",
        "      {\"role\": \"user\", \"content\": f\"User: {question}\"}\n",
        "  ]\n",
        "\n",
        "  res = generate_chat(messages) # perform API call\n",
        "\n",
        "  response = {\"model_response\": res}\n",
        "\n",
        "  # first we have to check if search is required for the question. It will happen when the model generate a search string\n",
        "  search_regex = r'{0} \\(search query\\):\\s\"([^\"]*)\"'.format(assistant_name)\n",
        "  search_match = re.search(search_regex, res)\n",
        "\n",
        "  if search_match: \n",
        "    \n",
        "    search_string = search_match.group(1) # get generated search string\n",
        "    response[\"search\"] = search_string\n",
        "\n",
        "  if do_search: # if search is enabled\n",
        "      search_results = search(search_string) # perform search\n",
        "\n",
        "      # construct the message containing the search results\n",
        "      search_engine_prompt = \"Search engine:\\n\"\n",
        "      for item in search_results:\n",
        "        search_engine_prompt += f\"Title: {item['title']}\\nURL: {item['link']}\\nText: {item['snippet']}\\n\\n\"\n",
        "      \n",
        "      # append the model last response to the messages\n",
        "      messages.append({\"role\": \"assistant\", \"content\":res})\n",
        "      # append the search results to the messages\n",
        "      messages.append({\"role\": \"user\",\"content\": search_engine_prompt})\n",
        "\n",
        "      # perform OpenAI API call\n",
        "      res = generate_chat(messages)\n",
        "      response[\"model_response_search\"] = res\n",
        "  \n",
        "  # Extract answer\n",
        "  answer_regex = r'{0}:\\s(.*)'.format(assistant_name)\n",
        "  answer_match = re.search(answer_regex, res)\n",
        "  if answer_match:\n",
        "    response['answer'] = answer_match.group(1) \n",
        "\n",
        "  # Extract model internal thoughts\n",
        "  thougths_regex = r\"{0} \\(internal thoughts\\):\\s(.*)\".format(assistant_name)\n",
        "  thougths_match = re.search(thougths_regex, res)\n",
        "  if thougths_match:\n",
        "    response['thougths'] = thougths_match.group(1) \n",
        "\n",
        "  return response"
      ],
      "metadata": {
        "id": "3PxW66WVeYwl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing\n",
        "\n",
        "Let's proceed with the testing of our assistant. Initially, we call the assistant function with the **`do_search`** parameter set to False, indicating that we want to skip the search step. The question we provide is \"Who is Jayr Alencar Pereira,\" which is a specific inquiry that goes beyond the expected knowledge of the GPT-3.5-turbo model.\n",
        "\n",
        "The response from the assistant reveals that it lacks information about Jayr Alencar Pereira in its internal knowledge bases. This suggests that the person might not be widely recognized or documented in public sources. However, the assistant expresses willingness to offer further assistance if the user can provide additional information or context about the person.\n",
        "\n",
        "Moreover, the assistant generates a search string, \"Jayr Alencar Pereira,\" which serves as a suggestion for conducting an online search to gather more information about the subject if desired."
      ],
      "metadata": {
        "id": "5K3krLykVTkJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display_markdown\n",
        "\n",
        "assistant_name = \"Wally\" # @param\n",
        "question = \"Who is Jayr Alencar Pereira?\" # @param\n",
        "results = assistant(question,assistant_name=assistant_name,do_search=False)\n",
        "\n",
        "display_markdown(f\"**{assistant_name} (internal thoughts)**: {results['thougths']}\", raw=True)\n",
        "display_markdown(f\"**{assistant_name}**: {results['answer']}\", raw=True)\n",
        "if \"search\" in results:\n",
        "  display_markdown(f\"**{assistant_name} (search string)**: {results['search']}\", raw=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "id": "QA3keW-Mr6Ug",
        "outputId": "4be419d6-57dd-4426-cbb8-fd621aa76998"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "**Wally (internal thoughts)**: The user is asking about a specific person named Jayr Alencar Pereira. I should follow rules 2 (informative), 4 (question assessment), and 7 (searching)."
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "**Wally**: I'm sorry, but I don't have any information about Jayr Alencar Pereira in my internal knowledge bases. It's possible that this person is not well-known or hasn't been documented in public sources. If you have any additional information or context about who this person is, I can try to help you find more information."
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "**Wally (search string)**: Jayr Alencar Pereira"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When the **`do_search`** parameter is set to True, we obtain the following results from the assistant.\n",
        "\n",
        "The assistant internal thoughts indicate that the search process provided results related to \"Jayr Alencar Pereira\"."
      ],
      "metadata": {
        "id": "5uLbFDf4eFJT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = assistant(question,assistant_name=assistant_name,do_search=True)\n",
        "\n",
        "display_markdown(f\"**{assistant_name} (internal thoughts)**: {results['thougths']}\",raw=True)\n",
        "display_markdown(f\"**{assistant_name}**: {results['answer']}\",raw=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "e82n3vO8WpZw",
        "outputId": "5ae572b6-aa15-47c9-c331-893067f5369d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "**Wally (internal thoughts)**: The search provided results related to a person named Jayr Alencar Pereira, who appears to be a PhD student and researcher in the field of computer science. I should follow rules 2 (informative), 6 (multi-aspect), and 8 (knowledge recitation)."
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "**Wally**: Based on my search, Jayr Alencar Pereira is a PhD student and researcher in the field of computer science. He is affiliated with the Federal University of Pernambuco and NeuralMind. His research interests include natural language processing, machine learning, and assistive technologies. Some of his recent publications include \"Visconde: Multi-document QA with GPT-3 and Neural Retrieval\" and \"Using Assistive Robotics for Aphasia Rehabilitation.\""
          },
          "metadata": {}
        }
      ]
    }
  ]
}
