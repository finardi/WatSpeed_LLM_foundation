{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/finardi/WatSpeed_LLM_foundation/blob/main/Module3%3A%20Data_augmentation_(enrichment)_with_GPT_3_5_turbo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data augmentation (enrichment) with GPT-3.5-turbo\n",
        "\n",
        "This notebook showcases the use of GPT-3.5-turbo for data augmentation by adding a new feature to the examples of the [Incomplete Information Reading Comprehension (IIRC)](https://allenai.org/data/iirc) dataset. The task is to provide an explanation of how the provided documents answer the given question, in addition to the original question and context. \n",
        "\n",
        "The IIRC dataset is a crowdsourced dataset that contains information-seeking questions, which require models to identify and retrieve necessary information that is missing from the original context. Each context is a paragraph from English Wikipedia and comes with a set of links to other Wikipedia pages, and answering the questions requires following the appropriate links and retrieving relevant information from those linked pages that is missing from the original context. \n",
        "\n",
        "The newly added feature will be helpful for using the dataset as few-shot examples that induce chain-of-thought, enabling models to learn to reason and make predictions based on incomplete information."
      ],
      "metadata": {
        "id": "RXl8f8w3CWq1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installing required packages\n",
        "\n",
        "In this example, we have to install `openai` and `tiktoken` libraries.\n",
        "\n",
        "**`openai`**:\n",
        "\n",
        "OpenAI is an artificial intelligence research laboratory consisting of the for-profit corporation OpenAI LP and its parent company, the non-profit OpenAI Inc. The OpenAI library is a powerful machine learning library that provides an easy-to-use interface to the OpenAI API. With this library, users can easily integrate OpenAI's state-of-the-art language models, including GPT-3, into their applications, and leverage the full power of these models to perform various natural language processing (NLP) tasks, such as language generation, classification, question-answering, and more.\n",
        "\n",
        "**`tiktoken`**:\n",
        "\n",
        "Tiktoken is an open-source BPE tokeniser developed by OpenAI that is used to split text strings into tokens. It is useful for models like GPT-3 that encode text into tokens. Tiktoken is designed to be highly efficient, capable of handling large amounts of text quickly and accurately."
      ],
      "metadata": {
        "id": "TdfYvxKdelHr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tUECguIHCQuN",
        "outputId": "5f0c3e0a-5af2-4984-8f68-1b5a9dc2f22f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting openai\n",
            "  Downloading openai-0.27.6-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.65.0)\n",
            "Collecting aiohttp (from openai)\n",
            "  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp->openai)\n",
            "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->openai)\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting yarl<2.0,>=1.0 (from aiohttp->openai)\n",
            "  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp->openai)\n",
            "  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp->openai)\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Installing collected packages: multidict, frozenlist, async-timeout, yarl, aiosignal, aiohttp, openai\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 frozenlist-1.3.3 multidict-6.0.4 openai-0.27.6 yarl-1.9.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2022.10.31)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.27.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
            "Installing collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip install openai\n",
        "!pip install tiktoken"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download dataset\n",
        "\n",
        "To run the data augmentation example using GPT-3.5-turbo, we will use the IIRC (Incomplete Information Reading Comprehension) dataset, which is available for download from the official website of the Allen Institute for Artificial Intelligence (AI2) at https://allenai.org/data/iirc. To download and extract the dataset, we can use the code below.\n",
        "\n",
        "This will download the dataset and extract it into the current directory."
      ],
      "metadata": {
        "id": "AXZPm6AwCng5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://iirc-dataset.s3.us-west-2.amazonaws.com/iirc_train_dev.tgz\n",
        "!tar zxvf iirc_train_dev.tgz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1fpoeaOnCuXR",
        "outputId": "edb4e3d5-7dde-492e-bfbc-2b207fc6f0bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-05-15 23:38:06--  https://iirc-dataset.s3.us-west-2.amazonaws.com/iirc_train_dev.tgz\n",
            "Resolving iirc-dataset.s3.us-west-2.amazonaws.com (iirc-dataset.s3.us-west-2.amazonaws.com)... 52.92.152.98, 52.218.181.169, 3.5.82.173, ...\n",
            "Connecting to iirc-dataset.s3.us-west-2.amazonaws.com (iirc-dataset.s3.us-west-2.amazonaws.com)|52.92.152.98|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5713428 (5.4M) [application/gzip]\n",
            "Saving to: ‘iirc_train_dev.tgz’\n",
            "\n",
            "iirc_train_dev.tgz  100%[===================>]   5.45M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2023-05-15 23:38:06 (37.9 MB/s) - ‘iirc_train_dev.tgz’ saved [5713428/5713428]\n",
            "\n",
            "._iirc_train_dev\n",
            "iirc_train_dev/\n",
            "iirc_train_dev/._dev.json\n",
            "iirc_train_dev/dev.json\n",
            "iirc_train_dev/._README\n",
            "iirc_train_dev/README\n",
            "iirc_train_dev/._train.json\n",
            "iirc_train_dev/train.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data preparation\n",
        "\n",
        "The code below loads the training data of the IIRC dataset, which is stored in the \"train.json\" file. It then processes the data to extract the questions and their corresponding answers, and stores them in a list. \n",
        "\n",
        "The script starts by loading the data from the \"train.json\" file and storing it in the \"dev\" variable. The \"dev\" variable is a list of dictionaries, where each dictionary represents an item from the dataset. Each item contains a paragraph of text from Wikipedia, along with a set of questions related to the text.\n",
        "\n",
        "The script then initializes an empty list called \"all_questions\". It loops through each item in the \"dev\" variable and loops through each question in the \"questions\" field of the item's dictionary. For each question, it extracts the title of the corresponding paragraph from the \"title\" field of the item's dictionary and stores it in the \"title\" field of the question's dictionary. \n",
        "\n",
        "The script then processes the answer field of the question's dictionary. Depending on the \"type\" field of the answer, it formats the answer as a string and stores it in the \"answer\" field of the question's dictionary. The different types of answers that can be encountered are \"span\", \"value\", \"binary\", and \"none\". \n",
        "\n",
        "Finally, the script appends the question's dictionary to the \"all_questions\" list and continues looping through the questions. The script then prints the length of the \"all_questions\" list, which is the total number of questions in the training data."
      ],
      "metadata": {
        "id": "RsfT-lHdfsII"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "dev = json.load(open(\"./iirc_train_dev/train.json\"))\n",
        "all_questions = []\n",
        "\n",
        "for item in dev:\n",
        "    for q in item['questions']:\n",
        "        q['title'] = item['title']\n",
        "        answer = \"\"\n",
        "        if q['answer']['type'] == \"span\":\n",
        "            answer = \", \".join([a['text'] for a in q['answer'][\"answer_spans\"]])\n",
        "        elif q['answer']['type'] == \"value\":\n",
        "            answer = \"{0} {1}\".format(q['answer']['answer_value'],q['answer']['answer_unit'])\n",
        "        elif q['answer']['type'] == \"binary\":\n",
        "            answer = q['answer']['answer_value']\n",
        "        elif q['answer']['type'] == \"none\":\n",
        "            answer = \"Not enough information.\"\n",
        "        q['answer'] = answer\n",
        "        all_questions.append(q)\n",
        "len(all_questions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BL2tUyJFC7bO",
        "outputId": "d18a12ce-e1b0-4cdc-9ab5-1caf6f6abe00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10839"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The prepared dataset format is a list of dictionaries, where each dictionary represents a single question-answer pair. The keys in the dictionary include:\n",
        "\n",
        "- **`context`**: a list containing a single dictionary representing the context or passage from which the question is being asked. The 'text' key in the dictionary contains the actual text of the passage, while the 'indices' key specifies the start and end indices of the passage within the full document.\n",
        "- **`question_links`**: a list of Wikipedia links that are relevant to answering the question.\n",
        "- **`answer`**: a string representing the answer to the question.\n",
        "- **`question`**: a string representing the question itself.\n",
        "- **`qid`**: a string representing the unique identifier of the question.\n",
        "- **`title`**: a string representing the title of the Wikipedia page that the context passage is taken from.\n"
      ],
      "metadata": {
        "id": "6tuKEg_tg7_N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_questions[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jh2MMQxvKVAi",
        "outputId": "b643c481-7e03-48da-8559-6122d8216170"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'context': [{'text': 'During Operation Market Garden, the attempt to seize a bridgehead across the Rhine in the Netherlands, the 704th dropped supplies to allied troops near Nijmegen.',\n",
              "   'indices': [494, 655],\n",
              "   'passage': 'main'},\n",
              "  {'text': 'Operation Market Garden was a failed World War II military operation fought in the Netherlands from 17 to 25 September 1944.',\n",
              "   'indices': [0, 124],\n",
              "   'passage': 'Operation Market Garden'}],\n",
              " 'question_links': ['Operation Market Garden'],\n",
              " 'answer': ' from 17 to 25 September 1944',\n",
              " 'question': 'When did the operation during which the 704th dropped supplies to allied troops near Nijmegen begin?',\n",
              " 'qid': 'q_0',\n",
              " 'title': '446th Operations Group'}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using OpenAI API\n",
        "\n",
        "To use OpenAI API, we need to set our API key and import the OpenAI module. In the given code, we have the `OPENAI_KEY` variable which we can set to our OpenAI API key. After that, we can use the `openai.api_key` method to set the API key for our session.\n",
        "\n",
        "The function `generate_chat` takes in a list of messages and generates a response using the OpenAI Chat API. The `model` parameter specifies which model to use for generating the response. In the given code, we have used the `gpt-3.5-turbo` model. However, we can also use `gpt-4`.\n",
        "\n",
        "**IMPORTANT:** It's important to note that there are costs associated with using the OpenAI API, so we need to choose the appropriate model and set the parameters carefully to avoid unnecessary expenses."
      ],
      "metadata": {
        "id": "u4AWQ6B2gdIi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "\n",
        "OPENAI_KEY = \"\" # @param set your OpenAI API key here\n",
        "\n",
        "openai.api_key = OPENAI_KEY\n",
        "\n",
        "def generate_chat(messages,model=\"gpt-3.5-turbo\"):\n",
        "  response = openai.ChatCompletion.create(\n",
        "    model=model,\n",
        "    messages=messages,\n",
        "    temperature=0\n",
        "  )\n",
        "  return response[\"choices\"][0]['message']['content']"
      ],
      "metadata": {
        "id": "1fWxwY7kRo0H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Augmentation\n",
        "\n",
        "The aim of this section is to generate explanations on how the documents of each example of the IIRC dataset can be used to answer the associated question. To achieve this, we will use GPT-3.5-turbo for data augmentation.\n",
        "\n",
        "The method we will use involves asking the model to generate explanations based on a prompt that includes three elements:\n",
        "\n",
        "1. An instruction on what the model should generate (in this case, an explanation on how to use the documents to answer the question).\n",
        "2. Few-shot examples (at least one) that provide the model with context and information about the task. These examples will be provided in the form of prompts that include both the question and the associated documents.\n",
        "3. The target example, which is the example we want to augment. This example will be provided in the form of a prompt that includes only the question.\n",
        "\n",
        "By combining these elements, we can generate new examples that include an explanation on how to use the documents to answer the question. This approach will help us to improve the IIRC dataset by providing more informative examples that induce a chain-of-thought and can be used as few-shot examples for downstream tasks."
      ],
      "metadata": {
        "id": "M9xH_QcNDOTd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The instruction\n",
        "\n",
        "The instruction is a template text that explains the task that the GPT-3.5-turbo model will perform during the data augmentation process. The instruction has three input parts that will be filled in with the actual data by the user:"
      ],
      "metadata": {
        "id": "Uyw3STqriuvg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "instruction = \"\"\"The user will provide you with:\n",
        "1) some content documents;\n",
        "2) a question that can be answered by these documents; and\n",
        "3) the correct answer to the question based on the documents.\n",
        "\n",
        "Your task is to write a reasoning paragraph that explains how the provided documents answer the question.\n",
        "\n",
        "Expected output:\n",
        "Explanation: <>\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "vyvBW2ZoDivo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## One-shot example\n",
        "\n",
        "The code below presents a one-shot example, which is a single example of a task for the IIRC model to solve. The example includes a list of four documents, a question that can be answered using the information in the documents, **the correct answer to the question** based on the documents, and an explanation of how the documents can be used to answer the question.\n",
        "\n",
        "**Additionally, you can add more examples or modify the explanation provided to the model, aiming to induce it to produce the desired result.**\n"
      ],
      "metadata": {
        "id": "jREo2Fpdj6QJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "examples = [\n",
        "    {\n",
        "      \"documents\": [\n",
        "          \"\\\"San Tropez\\\" is the fourth track from the album Meddle by the band Pink Floyd. This song was one of several to be considered for the band's \\\"best of\\\" album, Echoes: The Best of Pink Floyd.\",\n",
        "          \"The French Riviera (known in French as the Côte d'Azur [kot daˈzyʁ]; Occitan: Còsta d'Azur [ˈkɔstɔ daˈzyɾ]; literal translation \\\"Azure Coast\\\") is the Mediterranean coastline of the southeast corner of France. There is no official boundary, but it is usually considered to extend from Cassis, Toulon or Saint-Tropez on the west to Menton at the France–Italy border in the east, where the Italian Riviera joins. The coast is entirely within the Provence-Alpes-Côte d'Azur (Région Sud) region of France. The Principality of Monaco is a semi-enclave within the region, surrounded on three sides by France and fronting the Mediterranean.\",\n",
        "          \"Moon also promised transparency in his presidency, moving the presidential residence from the palatial and isolated Blue House to an existing government complex in downtown Seoul.\",\n",
        "          \"Saint-Tropez (US: /ˌsæn troʊˈpeɪ/ SAN-troh-PAY, French: [sɛ̃ tʁɔpe]; Occitan: Sant-Tropetz , pronounced [san(t) tʀuˈpes]) is a town on the French Riviera, 68 kilometres (42 miles) west of Nice and 100 kilometres (62 miles) east of Marseille in the Var department of the Provence-Alpes-Côte d'Azur region of Occitania, Southern France.\"\n",
        "      ],\n",
        "      \"question\": \"Did Pink Floyd have a song about the French Riviera?\",\n",
        "      \"answer\": \"yes\",\n",
        "      \"explanation\": \"According to Document 1, \\\"San Tropez\\\" is the fourth track from the album Meddle by the band Pink Floyd. Document 4 states that Saint-Tropez is a town on the French Riviera, which is a part of the Mediterranean coastline in the southeast corner of France, as mentioned in Document 2. Therefore, the song \\\"San Tropez\\\" by Pink Floyd is about a location on the French Riviera.\"\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "3397ObvaEx3l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate explanation\n",
        "\n",
        "The next step is to construct the entire prompt by adding the target example, and then run the API call. \n",
        "\n",
        "Before that, we need to load the GPT-3.5-turbo tokenizer using the `tiktoken` library. The tokenizer will be useful for estimating how much our implementation will cost when using the OpenAI API. "
      ],
      "metadata": {
        "id": "_PUUX93slD7r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "\n",
        "model = \"gpt-3.5-turbo\"\n",
        "tokenizer = tiktoken.encoding_for_model(model)"
      ],
      "metadata": {
        "id": "8GS8nY5TlLVm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once we have the tokenizer, we can construct the prompt by combining the previously defined instruction and example with the target example. The target example will include a question, documents, and the correct answer to the question based on the provided documents. Finally, we will execute the API call to generate the explanation paragraph for the given example.\n",
        "\n",
        "The **`generate_explanation`** function is responsible for generating an explanation for a given item, which consists of a context, a question, and the correct answer. The function takes in the **`item`** parameter, which is a dictionary containing the context, question, and answer for the target example. The function constructs a prompt based on the provided **`instruction`**, the **`examples`**, and the target example. \n",
        "\n",
        "The function then sends the constructed prompt to **`generate_chat`** function, which is responsible for generating a response from the GPT-3.5-turbo model. The output from the model is then parsed using a regular expression to extract the explanation. \n",
        "\n",
        "If the **`cost_estimation`** parameter is set to **`True`**, the function will estimate the cost of generating the prompt using the GPT-3.5-turbo API based on the number of tokens in the prompt. The function returns the generated explanation if it is found, or **`None`** otherwise."
      ],
      "metadata": {
        "id": "2FSNnSbtlKza"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def generate_explanation(item, cost_estimation=False):\n",
        "  messages = [{\"role\":\"system\",\"content\":instruction}]\n",
        "\n",
        "  for example in examples:\n",
        "    docs_str = \"\"\n",
        "    for i,doc in enumerate(example['documents']):\n",
        "      docs_str += f\"[Document {i+1}]: {doc}\\n##\\n\"\n",
        "    \n",
        "    messages.append({\"role\":\"user\",\"content\":f\"{docs_str}Question: {example['question']}\\nAnswer: {example['answer']}\"})\n",
        "\n",
        "  target_docs_str = \"\"\n",
        "\n",
        "  for i, doc in enumerate(item['context']):\n",
        "    target_docs_str += f\"[Document {i+1}]: {doc['text']}\\n##\\n\"\n",
        "  \n",
        "  #  Note that we include the correct answer as an input to the model, thus making its job model easier.\n",
        "  messages.append({\"role\":\"user\",\"content\":f\"{target_docs_str}Question: {item['question']}\\nAnswer: {item['answer']}\"})\n",
        "\n",
        "  if cost_estimation:\n",
        "    prompt = \"\\n\".join([message[\"content\"] for message in messages])\n",
        "    tokens = tokenizer.encode(prompt)\n",
        "    output_size = 128 #128 tokens\n",
        "    return ((len(tokens) + output_size) / 1000) * 0.002\n",
        "  \n",
        "  res = generate_chat(messages)\n",
        "\n",
        "  regex = r\"Explanation:\\s(.*)\"\n",
        "\n",
        "  match = re.search(regex, res)\n",
        "\n",
        "  if match:\n",
        "      return match.group(1) "
      ],
      "metadata": {
        "id": "fjQVGihxFVqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cost Estimation\n",
        "\n",
        "The code below performs a cost estimation for the generation of explanations for a set of questions. The `limit` variable defines the number of questions to consider."
      ],
      "metadata": {
        "id": "qZ9VhUOlmeqe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "limit = 100\n",
        "costs = []\n",
        "\n",
        "for question in all_questions[:limit]:\n",
        "  costs.append(generate_explanation(question, cost_estimation=True))\n",
        "\n",
        "print(f\"Generating explanations for {limit} questions will cost U$ {np.sum(costs):.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BFyd_vO5Kzht",
        "outputId": "8f2bfc06-2dd9-4982-a9fd-689ede26fe10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating explanations for 100 questions will cost U$ 0.14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running\n",
        "\n",
        "Before generating the explanations, we use the code above to randomly sample a fixed number of examples from the training dataset. This is done to avoid running the explanation generation process on the entire dataset, which could be computationally expensive. The **`random.sample`** function is used to randomly select examples from the list of all questions **`all_questions`**, and the number of examples to be selected is specified by the variable **`limit`**. After sampling the examples, we define the variable **`explained_dataset`**, which will store the explained questions generated by our model."
      ],
      "metadata": {
        "id": "G0nBdn_am6ah"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "sampled_examples = random.sample(all_questions, k=limit)\n",
        "\n",
        "explained_dataset = []"
      ],
      "metadata": {
        "id": "faNNiaZ0POnG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above code block is a for loop that iterates over each of the randomly sampled questions (stored in the **`sampled_examples`** list) and generates an explanation for them using the **`generate_explanation()`** function. The **`tqdm()`** function provides a progress bar to show the progress of the for loop. The generated explanations are then stored in the **`explanation`** field of each question dictionary and the updated question dictionary is appended to the **`explained_dataset`** list. \n",
        "\n",
        "The length of the **`explained_dataset`** list is used as an index to the **`sampled_examples`** list to ensure that we do not generate explanations for the same questions more than once. This is necessary to avoid overloading the OpenAI API and to ensure that we are not wasting resources generating redundant explanations."
      ],
      "metadata": {
        "id": "dJU7AuzloPov"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "for question in tqdm(sampled_examples[len(explained_dataset):]):\n",
        "  question[\"explanation\"] = generate_explanation(question)  \n",
        "  explained_dataset.append(question)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CgnJieB7JzVg",
        "outputId": "99228b90-6ef7-4bcf-8921-59036587b4dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [02:42<00:00,  1.62s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code saves the explained questions with their corresponding explanations to a JSON file. The code uses the **`json`** library to encode the list of questions and their explanations to a JSON object, which is then written to a file named **`explained_dataset.json`**. The **`with open()`** block ensures that the file is properly closed after writing the JSON object to it. \n",
        "\n",
        "❗**NOTE**❗: please, download this generated file from the Colab notebook since it will be necessary for the lab assignment notebook.\n",
        "\n",
        "If you prefer, you can use [this version](https://drive.google.com/file/d/11QOpNF9PoANSli0MAUKTeYq_TfhYE6sg/view?usp=share_link)."
      ],
      "metadata": {
        "id": "_NX2uOfQofHs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open('explained_dataset.json', 'w') as f:\n",
        "  json.dump(explained_dataset, f)"
      ],
      "metadata": {
        "id": "VH5Ww2U8OXpY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}